{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from jiwer import wer, cer\n",
    "from time import time\n",
    "import pandas as pd\n",
    "from typing import Tuple, Dict, List\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchaudio\n",
    "\n",
    "# setting the random seed for reproducibility\n",
    "SEED = 2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomSpeechDataset(torch.utils.data.Dataset):\n",
    "    \n",
    "    \"\"\"\n",
    "    Custom torch dataset class to load the dataset \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, manifest_file: str, audio_dir: str, is_test_set: bool=False) -> None:\n",
    "      \n",
    "        \"\"\"\n",
    "        To initialise Custom dataset object:\n",
    "        \n",
    "        - manifest_file:    json file that contains the filename of the audio, and also the annotation if is_test_set is set to False\n",
    "        - audio_dir:        root directory of the audio datasets\n",
    "        - is_test_set:      flag variable to switch between loading of the train and the test set. Train set loads the annotation whereas test set does not\n",
    "        \"\"\"\n",
    "\n",
    "        self.audio_dir = audio_dir\n",
    "        self.is_test_set = is_test_set\n",
    "\n",
    "        with open(manifest_file, 'r') as f:\n",
    "            self.manifest = json.load(f)\n",
    "\n",
    "        \n",
    "    def __len__(self) -> int:\n",
    "        \n",
    "        \"\"\"\n",
    "        To get the number of loaded audio files in the dataset\n",
    "        \"\"\"\n",
    "\n",
    "        return len(self.manifest)\n",
    "    \n",
    "    \n",
    "    def __getitem__(self, index: int) -> Tuple[str, torch.Tensor]:\n",
    "\n",
    "        \"\"\"\n",
    "        To get the values required to do the training:\n",
    "        \n",
    "        Takes in an int index to return tuple of the audio path, \n",
    "        audio signal & annotation.       \n",
    "        \"\"\"\n",
    "\n",
    "        if torch.is_tensor(index):\n",
    "            index.tolist()\n",
    "            \n",
    "        audio_path = self._get_audio_path(index)\n",
    "        signal, sr = torchaudio.load(audio_path)\n",
    "        \n",
    "        if not self.is_test_set:\n",
    "            annotation = self._get_annotation(index)\n",
    "            return audio_path, signal, annotation\n",
    "        \n",
    "        return audio_path, signal\n",
    "    \n",
    "    \n",
    "    def _get_audio_path(self, index: int) -> str:\n",
    "\n",
    "        \"\"\"\n",
    "        Helper function to retrieve the audio path from the json manifest file\n",
    "        \"\"\"\n",
    "        audio_filename = os.path.basename(self.manifest[index]['path'])\n",
    "        audio_path = os.path.join(self.audio_dir, audio_filename)\n",
    "        return audio_path\n",
    "    \n",
    "    \n",
    "    def _get_annotation(self, index: int) -> str:\n",
    "\n",
    "        \"\"\"\n",
    "        Helper function to retrieve the annotation from the json manifest file\n",
    "        \"\"\"\n",
    "\n",
    "        return self.manifest[index]['annotation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextTransform:\n",
    "\n",
    "    \"\"\"\n",
    "    Map characters to integers and vice versa (encoding/decoding)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self) -> None:\n",
    "\n",
    "        char_map_str = \"\"\"\n",
    "            <SPACE> 0\n",
    "            A 1\n",
    "            B 2\n",
    "            C 3\n",
    "            D 4\n",
    "            E 5\n",
    "            F 6\n",
    "            G 7\n",
    "            H 8\n",
    "            I 9\n",
    "            J 10\n",
    "            K 11\n",
    "            L 12\n",
    "            M 13\n",
    "            N 14\n",
    "            O 15\n",
    "            P 16\n",
    "            Q 17\n",
    "            R 18\n",
    "            S 19\n",
    "            T 20\n",
    "            U 21\n",
    "            V 22\n",
    "            W 23\n",
    "            X 24\n",
    "            Y 25\n",
    "            Z 26\n",
    "        \"\"\"\n",
    "        \n",
    "        self.char_map = {}\n",
    "        self.index_map = {}\n",
    "        \n",
    "        for line in char_map_str.strip().split('\\n'):\n",
    "            ch, index = line.split()\n",
    "            self.char_map[ch] = int(index)\n",
    "            self.index_map[int(index)] = ch\n",
    "\n",
    "        self.index_map[0] = ' '\n",
    "\n",
    "\n",
    "    def get_char_len(self) -> int:\n",
    "\n",
    "        \"\"\"\n",
    "        Gets the no. of characters that are being encoded & decoded in the prediction\n",
    "        (the char_map string).\n",
    "        Useful for defining the output dimension of the speech recognition model.\n",
    "        Returns:\n",
    "        --------\n",
    "            the number of characters defined in the __init__ char_map_str\n",
    "        \"\"\"\n",
    "\n",
    "        return len(self.char_map)\n",
    "    \n",
    "\n",
    "    def get_char_list(self) -> List[str]:\n",
    "\n",
    "        \"\"\"\n",
    "        Gets the list of characters that are being encoded and decoded in the prediction\n",
    "        (defined in the char_map string).\n",
    "        Useful for displaying the data's character set.\n",
    "        \n",
    "        Returns:\n",
    "        -------\n",
    "            a list of characters defined in the __init__ char_map_str\n",
    "        \"\"\"\n",
    "\n",
    "        return list(self.index_map.values())\n",
    "    \n",
    "\n",
    "    def text_to_int(self, text: str) -> List[int]:\n",
    "\n",
    "        \"\"\"\n",
    "        Use a character map and convert text to an integer encoded sequence \n",
    "                \n",
    "        Returns:\n",
    "        -------\n",
    "            a list of the text encoded to an integer sequence \n",
    "        \"\"\"\n",
    "        \n",
    "        int_sequence = []\n",
    "        for c in text:\n",
    "            if c == ' ':\n",
    "                ch = self.char_map['<SPACE>']\n",
    "            else:\n",
    "                ch = self.char_map[c]\n",
    "            int_sequence.append(ch)\n",
    "\n",
    "        return int_sequence\n",
    "    \n",
    "\n",
    "    def int_to_text(self, labels) -> str:\n",
    "\n",
    "        \"\"\"\n",
    "        Use a character map and convert integer labels to an text sequence \n",
    "        \n",
    "        Returns:\n",
    "        -------\n",
    "            the decoded transcription\n",
    "        \"\"\"\n",
    "        \n",
    "        string = []\n",
    "        for i in labels:\n",
    "            string.append(self.index_map[i])\n",
    "\n",
    "        return ''.join(string).replace('<SPACE>', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GreedyDecoder:\n",
    "\n",
    "    \"\"\"\n",
    "    Decodes the logits into characters to form the final transciption using the greedy decoding approach\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        pass\n",
    "\n",
    "\n",
    "    def decode(\n",
    "            self, \n",
    "            output: torch.Tensor, \n",
    "            labels: torch.Tensor=None,      #only for non-test set with annotations\n",
    "            label_lengths: List[int]=None, \n",
    "            collapse_repeated: bool=True, \n",
    "            is_test: bool=False\n",
    "        ):\n",
    "        \n",
    "        \"\"\"\n",
    "        Main method to call for the decoding of the text from the predicted logits (output probabilities).\n",
    "        \"\"\"\n",
    "        \n",
    "        text_transform = TextTransform()\n",
    "        arg_maxes = torch.argmax(output, dim=2)\n",
    "        decodes = []\n",
    "\n",
    "        # refer to char_map_str in the TextTransform class -> only have index from 0 to 26, hence 27 represents the case where the character is decoded as blank (NOT <SPACE>)\n",
    "        decoded_blank_idx = text_transform.get_char_len()\n",
    "\n",
    "        if not is_test:\n",
    "            targets = []\n",
    "\n",
    "        for i, args in enumerate(arg_maxes):\n",
    "            decode = []\n",
    "\n",
    "            if not is_test:\n",
    "                targets.append(text_transform.int_to_text(labels[i][:label_lengths[i]].tolist()))\n",
    "\n",
    "            for j, char_idx in enumerate(args):\n",
    "                if char_idx != decoded_blank_idx:\n",
    "                    if collapse_repeated and j != 0 and char_idx == args[j-1]:\n",
    "                        continue\n",
    "                    decode.append(char_idx.item())\n",
    "            decodes.append(text_transform.int_to_text(decode))\n",
    "\n",
    "        return decodes, targets if not is_test else decodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataProcessor:\n",
    "\n",
    "    \"\"\"\n",
    "    Transforms the audio waveform tensors into a melspectrogram\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    def _audio_transformation(self, is_train: bool=True):\n",
    "\n",
    "        \"\"\"\n",
    "        Returns a sequence of audio transformation to be applied to the waveform tensor\n",
    "        if is_train = true, it returns a sequence of transformation including Mel Spectrogram,\n",
    "        frequency masking & time masking, otherwise it only returns the Mel spectrogram transformation.\n",
    "        \"\"\"\n",
    "\n",
    "        return torch.nn.Sequential(\n",
    "                torchaudio.transforms.MelSpectrogram(sample_rate=16000, n_mels=100), #reduced n_mels from 128 -> 100\n",
    "                torchaudio.transforms.FrequencyMasking(freq_mask_param=30),\n",
    "                torchaudio.transforms.TimeMasking(time_mask_param=100)\n",
    "            ) if is_train else torchaudio.transforms.MelSpectrogram(sample_rate=16000, n_mels=100)\n",
    "    \n",
    "\n",
    "    def data_processing(self, data, data_type='train'):\n",
    "\n",
    "        \"\"\"\n",
    "        Process the audio data to retrieve the spectrograms that will be used for training, testing & validation.\n",
    "        \n",
    "        For training & validation, it creates a list of labels & label_lens,\n",
    "        For testing, it creates a list of audio path lists.\n",
    "        \"\"\"\n",
    "\n",
    "        text_transform = TextTransform()\n",
    "        spectrograms = []\n",
    "        input_lengths = []\n",
    "        audio_path_list = []\n",
    "\n",
    "        audio_transforms = self._audio_transformation(is_train=True) if data_type == 'train' else self._audio_transformation(is_train=False)\n",
    "\n",
    "        if data_type != 'test':  \n",
    "            labels = []\n",
    "            label_lengths = []\n",
    "\n",
    "            for audio_path, waveform, utterance in data:\n",
    "\n",
    "                spec = audio_transforms(waveform).squeeze(0).transpose(0, 1)\n",
    "                spectrograms.append(spec)\n",
    "                label = torch.Tensor(text_transform.text_to_int(utterance))\n",
    "                labels.append(label)\n",
    "                input_lengths.append(spec.shape[0]//2)\n",
    "                label_lengths.append(len(label))\n",
    "\n",
    "            spectrograms = torch.nn.utils.rnn.pad_sequence(spectrograms, batch_first=True).unsqueeze(1).transpose(2, 3)\n",
    "            labels = torch.nn.utils.rnn.pad_sequence(labels, batch_first=True)\n",
    "            return audio_path, spectrograms, labels, input_lengths, label_lengths\n",
    "\n",
    "        else:\n",
    "            for audio_path, waveform in data:\n",
    "\n",
    "                spec = audio_transforms(waveform).squeeze(0).transpose(0, 1)\n",
    "                spectrograms.append(spec)\n",
    "                input_lengths.append(spec.shape[0]//2)\n",
    "                audio_path_list.append(audio_path)\n",
    "\n",
    "            spectrograms = torch.nn.utils.rnn.pad_sequence(spectrograms, batch_first=True).unsqueeze(1).transpose(2, 3)\n",
    "            return audio_path_list, spectrograms, input_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "building the model with adaption of deepspeech2 -> https://arxiv.org/abs/1512.02595\n",
    "\n",
    "code adapted from https://towardsdatascience.com/customer-case-study-building-an-end-to-end-speech-recognition-model-in-pytorch-with-assemblyai-473030e47c7c\n",
    "\"\"\"\n",
    "\n",
    "class CNNLayerNorm(torch.nn.Module):\n",
    "    \n",
    "    \"\"\"\n",
    "    Layer normalization built for CNNs input\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_feats: int) -> None:   #n_feats = no. of features\n",
    "        super(CNNLayerNorm, self).__init__()\n",
    "\n",
    "        self.layer_norm = torch.nn.LayerNorm(n_feats)\n",
    "\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Input x of dimension -> (batch, channel, feature, time)\n",
    "        \"\"\"\n",
    "        \n",
    "        x = x.transpose(2, 3).contiguous() # (batch, channel, time, feature)\n",
    "        x = self.layer_norm(x)\n",
    "\n",
    "        return x.transpose(2, 3).contiguous() # (batch, channel, feature, time) \n",
    "\n",
    "\n",
    "class ResidualCNN(torch.nn.Module):\n",
    "\n",
    "    \"\"\"\n",
    "    Residual CNN inspired by https://arxiv.org/pdf/1603.05027.pdf except with layer norm instead of batch norm\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, in_channels: int, out_channels: int, kernel: int, stride: int, dropout: float, n_feats: int) -> None:\n",
    "        super(ResidualCNN, self).__init__()\n",
    "\n",
    "        self.cnn1 = torch.nn.Conv2d(in_channels, out_channels, kernel, stride, padding=kernel//2)\n",
    "        self.cnn2 = torch.nn.Conv2d(out_channels, out_channels, kernel, stride, padding=kernel//2)\n",
    "        self.dropout1 = torch.nn.Dropout(dropout)\n",
    "        self.dropout2 = torch.nn.Dropout(dropout)\n",
    "        self.layer_norm1 = CNNLayerNorm(n_feats)\n",
    "        self.layer_norm2 = CNNLayerNorm(n_feats)\n",
    "\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \n",
    "        \"\"\"\n",
    "        Model building for the Residual CNN layers\n",
    "        \n",
    "        Input x of dimension -> (batch, channel, feature, time)\n",
    "        \"\"\"\n",
    "\n",
    "        residual = x\n",
    "        x = self.layer_norm1(x)\n",
    "        x = F.gelu(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.cnn1(x)\n",
    "        x = self.layer_norm2(x)\n",
    "        x = F.gelu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.cnn2(x)\n",
    "        x += residual\n",
    "\n",
    "        return x # (batch, channel, feature, time)\n",
    "\n",
    "\n",
    "class BidirectionalGRU(torch.nn.Module):\n",
    "\n",
    "    \"\"\"\n",
    "    The Bidirectional GRU composite code block which will be used in the main SpeechRecognitionModel class\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, rnn_dim: int, hidden_size: int, dropout: int, batch_first: int) -> None:\n",
    "        super(BidirectionalGRU, self).__init__()\n",
    "\n",
    "        self.BiGRU = torch.nn.GRU(\n",
    "            input_size=rnn_dim, \n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=1, \n",
    "            batch_first=batch_first, \n",
    "            bidirectional=True\n",
    "        )\n",
    "        self.layer_norm = torch.nn.LayerNorm(rnn_dim)\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "\n",
    "        \"\"\"\n",
    "        Transformation of the layers in the Bidirectional GRU block\n",
    "        \"\"\"\n",
    "\n",
    "        x = self.layer_norm(x)\n",
    "        x = F.gelu(x)\n",
    "        x, _ = self.BiGRU(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class SpeechRecognitionModel(torch.nn.Module):\n",
    "\n",
    "    \"\"\"\n",
    "    The main ASR Model that the main code will interact with\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_cnn_layers, n_rnn_layers, rnn_dim, n_class, n_feats, stride=2, dropout=0.1) -> None:\n",
    "        super(SpeechRecognitionModel, self).__init__()\n",
    "        \n",
    "        n_feats = n_feats//2\n",
    "        self.cnn = torch.nn.Conv2d(1, 32, 3, stride=stride, padding=3//2)  # cnn for extracting heirachal features\n",
    "\n",
    "        # n residual cnn layers with filter size of 32\n",
    "        self.rescnn_layers = torch.nn.Sequential(*[\n",
    "            ResidualCNN(\n",
    "                in_channels=32, \n",
    "                out_channels=32, \n",
    "                kernel=3, \n",
    "                stride=1, \n",
    "                dropout=dropout, \n",
    "                n_feats=n_feats\n",
    "            ) for _ in range(n_cnn_layers)\n",
    "        ])\n",
    "        self.fully_connected = torch.nn.Linear(n_feats*32, rnn_dim)\n",
    "        self.birnn_layers = torch.nn.Sequential(*[\n",
    "            BidirectionalGRU(\n",
    "                rnn_dim=rnn_dim if i==0 else rnn_dim*2,\n",
    "                hidden_size=rnn_dim, \n",
    "                dropout=dropout, \n",
    "                batch_first=i==0\n",
    "            ) for i in range(n_rnn_layers)\n",
    "        ])\n",
    "        self.classifier = torch.nn.Sequential(\n",
    "            torch.nn.Linear(rnn_dim*2, rnn_dim),  # birnn returns rnn_dim*2\n",
    "            torch.nn.GELU(),\n",
    "            torch.nn.Dropout(dropout),\n",
    "            torch.nn.Linear(rnn_dim, n_class)\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "\n",
    "        \"\"\"\n",
    "        Transformation of the layers in the ASR model block\n",
    "        \"\"\"\n",
    "\n",
    "        x = self.cnn(x)\n",
    "        x = self.rescnn_layers(x)\n",
    "        sizes = x.size()\n",
    "        x = x.view(sizes[0], sizes[1] * sizes[2], sizes[3])  # (batch, feature, time)\n",
    "        x = x.transpose(1, 2) # (batch, time, feature)\n",
    "        x = self.fully_connected(x)\n",
    "        x = self.birnn_layers(x)\n",
    "        x = self.classifier(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IterMeter(object):\n",
    "\n",
    "    \"\"\"\n",
    "    Keeps track of the total iterations during the training and validation loop\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self) -> None:\n",
    "        self.val = 0\n",
    "\n",
    "\n",
    "    def step(self):\n",
    "        self.val += 1\n",
    "\n",
    "\n",
    "    def get(self):\n",
    "        return self.val\n",
    "    \n",
    "\n",
    "class TrainingLoop:\n",
    "\n",
    "    \"\"\"\n",
    "    The main class to set up the training loop to train the model\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        pass\n",
    "    \n",
    "\n",
    "    def train(self, model, device, train_loader, criterion, optimizer, scheduler, epoch, iter_meter) -> None:\n",
    "\n",
    "        \"\"\"\n",
    "        Training Loop:\n",
    "        \n",
    "        Set the model into training mode and iterates through the training dataset using the train_loader. \n",
    "        For each batch, it moves the input tensors to the specified device, computes the model output,\n",
    "        calculate the loss in the provided criterion and updates the model weights using the optimizer.\n",
    "        The iter_meter is incremented at each step and the training progress is printed periodically.\n",
    "        \"\"\"\n",
    "        \n",
    "        model.train()\n",
    "        data_len = len(train_loader.dataset)\n",
    "        \n",
    "        for batch_idx, _data in enumerate(train_loader):\n",
    "            audio_path, spectrograms, labels, input_lengths, label_lengths = _data \n",
    "            spectrograms, labels = spectrograms.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            output = model(spectrograms)  # (batch, time, n_class)\n",
    "            output = F.log_softmax(output, dim=2)\n",
    "            output = output.transpose(0, 1) # (time, batch, n_class)\n",
    "\n",
    "            loss = criterion(output, labels, input_lengths, label_lengths)\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "            iter_meter.step()\n",
    "            \n",
    "            if batch_idx % 100 == 0 or batch_idx == data_len:\n",
    "                print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                    epoch, batch_idx * len(spectrograms), data_len,\n",
    "                    100. * batch_idx / len(train_loader), loss.item()))\n",
    "\n",
    "\n",
    "    def dev(self, model, device, dev_loader, criterion, scheduler, epoch, iter_meter) -> None:\n",
    "\n",
    "        \"\"\"\n",
    "        Validation Loop:\n",
    "        \n",
    "        Sets the model to evaluation mode and iterates through the validation dataset using dev_loader.\n",
    "        For each batch, it computes the model outpu, calculates the loss using the provided criterion and\n",
    "        accumulates the validation loss. It also calculates the character error rate (CER) & word error rate (WER)\n",
    "        for each decoded prediction and target pair.\n",
    "        \n",
    "        After processing all batches, the method computes the average validation loss, CER & WER. \n",
    "        It also adjusts the learning rate on the scheduler and prints the results.\n",
    "        \"\"\"\n",
    "        \n",
    "        print('\\nevaluating...')\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        test_cer, test_wer = [], []\n",
    "        greedy_decoder = GreedyDecoder()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for i, _data in enumerate(dev_loader):\n",
    "                audio_path, spectrograms, labels, input_lengths, label_lengths = _data \n",
    "                spectrograms, labels = spectrograms.to(device), labels.to(device)\n",
    "\n",
    "                output = model(spectrograms)  # (batch, time, n_class)\n",
    "                output = F.log_softmax(output, dim=2)\n",
    "                output = output.transpose(0, 1) # (time, batch, n_class)\n",
    "\n",
    "                loss = criterion(output, labels, input_lengths, label_lengths)\n",
    "                val_loss += loss.item() / len(dev_loader)\n",
    "\n",
    "                decoded_preds, decoded_targets = greedy_decoder.decode(output.transpose(0, 1), labels=labels, label_lengths=label_lengths, is_test=False)\n",
    "                # print(decoded_preds)\n",
    "                # print(decoded_targets)\n",
    "                \n",
    "                for j in range(len(decoded_preds)):\n",
    "                    test_cer.append(cer(decoded_targets[j], decoded_preds[j]))\n",
    "                    test_wer.append(wer(decoded_targets[j], decoded_preds[j]))\n",
    "\n",
    "        avg_cer = sum(test_cer)/len(test_cer)\n",
    "        avg_wer = sum(test_wer)/len(test_wer)\n",
    "        \n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        print('Dev set: Average loss: {:.4f}, Average CER: {:4f} Average WER: {:.4f}\\n'.format(val_loss, avg_cer, avg_wer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(hparams, train_dataset, dev_dataset, saved_model_path) -> None:\n",
    "\n",
    "    \"\"\"\n",
    "    The main method to call to do model training\n",
    "    \n",
    "    First checks if the GPU is available for training and that is set the random seed\n",
    "    for reproducibility.\n",
    "    \"\"\" \n",
    "\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    torch.manual_seed(SEED)\n",
    "    \n",
    "    data_processor = DataProcessor()\n",
    "    iter_meter = IterMeter()\n",
    "    text_transform = TextTransform()\n",
    "    trainer = TrainingLoop()\n",
    "    \n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "    kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\n",
    "    \n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        dataset=train_dataset,\n",
    "        batch_size=hparams['batch_size'],\n",
    "        shuffle=True,\n",
    "        collate_fn=lambda x: data_processor.data_processing(x, 'train'),\n",
    "        **kwargs\n",
    "    )\n",
    "    \n",
    "    dev_loader = torch.utils.data.DataLoader(\n",
    "        dataset=dev_dataset,\n",
    "        batch_size=hparams['batch_size'],\n",
    "        shuffle=False,\n",
    "        collate_fn=lambda x: data_processor.data_processing(x, 'dev'),\n",
    "        **kwargs\n",
    "    )\n",
    "\n",
    "    model = SpeechRecognitionModel(\n",
    "        hparams['n_cnn_layers'], \n",
    "        hparams['n_rnn_layers'], \n",
    "        hparams['rnn_dim'],\n",
    "        hparams['n_class'], \n",
    "        hparams['n_feats'], \n",
    "        hparams['stride'], \n",
    "        hparams['dropout']\n",
    "    ).to(device)\n",
    "\n",
    "    print('Num Model Parameters', sum([param.nelement() for param in model.parameters()]))\n",
    "\n",
    "    #prints total no. of model parameters & creates the optimizer function (CTC Loss) and the learning rates scheduler\n",
    "    \n",
    "    optimizer = torch.optim.AdamW(model.parameters(), hparams['learning_rate'])\n",
    "    criterion = torch.nn.CTCLoss(blank=text_transform.get_char_len()).to(device)\n",
    "    \n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer, mode='min', patience=3, verbose=True, factor=0.05)\n",
    "        \n",
    "    #Loops through a specified no. of epochs & call trainer.dev method of the training loop instance to train the model & evaluate on the development dataset.\n",
    "    for epoch in range(1, hparams['epochs'] + 1):\n",
    "        trainer.train(model, device, train_loader, criterion, optimizer, scheduler, epoch, iter_meter)\n",
    "        trainer.dev(model, device, dev_loader, criterion, scheduler, epoch, iter_meter)\n",
    "        \n",
    "    #Save the trained model\n",
    "    torch.save(model.state_dict(), saved_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Model Parameters 6129780\n",
      "Train Epoch: 1 [0/3000 (0%)]\tLoss: 8.344914\n",
      "Train Epoch: 1 [800/3000 (27%)]\tLoss: 2.919180\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 38\u001b[0m\n\u001b[0;32m     35\u001b[0m start_time \u001b[39m=\u001b[39m time()\n\u001b[0;32m     37\u001b[0m \u001b[39m#Start training the model\u001b[39;00m\n\u001b[1;32m---> 38\u001b[0m main(\n\u001b[0;32m     39\u001b[0m     hparams\u001b[39m=\u001b[39;49mhparams, \n\u001b[0;32m     40\u001b[0m     train_dataset\u001b[39m=\u001b[39;49mdataset_train, \n\u001b[0;32m     41\u001b[0m     dev_dataset\u001b[39m=\u001b[39;49mdataset_dev, \n\u001b[0;32m     42\u001b[0m     saved_model_path\u001b[39m=\u001b[39;49mSAVED_MODEL_PATH\n\u001b[0;32m     43\u001b[0m )\n\u001b[0;32m     45\u001b[0m end_time \u001b[39m=\u001b[39m time()\n\u001b[0;32m     47\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mTime taken for training: \u001b[39m\u001b[39m{\u001b[39;00m(end_time\u001b[39m-\u001b[39mstart_time)\u001b[39m/\u001b[39m(\u001b[39m60\u001b[39m\u001b[39m*\u001b[39m\u001b[39m60\u001b[39m)\u001b[39m}\u001b[39;00m\u001b[39m hrs\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[8], line 58\u001b[0m, in \u001b[0;36mmain\u001b[1;34m(hparams, train_dataset, dev_dataset, saved_model_path)\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[39m#Loops through a specified no. of epochs & call trainer.dev method of the training loop instance to train the model & evaluate on the development dataset.\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, hparams[\u001b[39m'\u001b[39m\u001b[39mepochs\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m):\n\u001b[1;32m---> 58\u001b[0m     trainer\u001b[39m.\u001b[39;49mtrain(model, device, train_loader, criterion, optimizer, scheduler, epoch, iter_meter)\n\u001b[0;32m     59\u001b[0m     trainer\u001b[39m.\u001b[39mdev(model, device, dev_loader, criterion, scheduler, epoch, iter_meter)\n\u001b[0;32m     61\u001b[0m \u001b[39m#Save the trained model\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[7], line 49\u001b[0m, in \u001b[0;36mTrainingLoop.train\u001b[1;34m(self, model, device, train_loader, criterion, optimizer, scheduler, epoch, iter_meter)\u001b[0m\n\u001b[0;32m     45\u001b[0m spectrograms, labels \u001b[39m=\u001b[39m spectrograms\u001b[39m.\u001b[39mto(device), labels\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m     47\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m---> 49\u001b[0m output \u001b[39m=\u001b[39m model(spectrograms)  \u001b[39m# (batch, time, n_class)\u001b[39;00m\n\u001b[0;32m     50\u001b[0m output \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mlog_softmax(output, dim\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n\u001b[0;32m     51\u001b[0m output \u001b[39m=\u001b[39m output\u001b[39m.\u001b[39mtranspose(\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m) \u001b[39m# (time, batch, n_class)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\Downloads\\NUS\\Brainhack 2023 - Today I Learned Artificial Intelligence (TIL-AI)\\Brainhack_2023\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[6], line 155\u001b[0m, in \u001b[0;36mSpeechRecognitionModel.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    153\u001b[0m x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mtranspose(\u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m) \u001b[39m# (batch, time, feature)\u001b[39;00m\n\u001b[0;32m    154\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfully_connected(x)\n\u001b[1;32m--> 155\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbirnn_layers(x)\n\u001b[0;32m    156\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclassifier(x)\n\u001b[0;32m    158\u001b[0m \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\Downloads\\NUS\\Brainhack 2023 - Today I Learned Artificial Intelligence (TIL-AI)\\Brainhack_2023\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\Downloads\\NUS\\Brainhack 2023 - Today I Learned Artificial Intelligence (TIL-AI)\\Brainhack_2023\\.venv\\lib\\site-packages\\torch\\nn\\modules\\container.py:139\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    137\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[0;32m    138\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[1;32m--> 139\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[0;32m    140\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\Downloads\\NUS\\Brainhack 2023 - Today I Learned Artificial Intelligence (TIL-AI)\\Brainhack_2023\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[6], line 97\u001b[0m, in \u001b[0;36mBidirectionalGRU.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     95\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer_norm(x)\n\u001b[0;32m     96\u001b[0m x \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mgelu(x)\n\u001b[1;32m---> 97\u001b[0m x, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mBiGRU(x)\n\u001b[0;32m     98\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(x)\n\u001b[0;32m    100\u001b[0m \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\Downloads\\NUS\\Brainhack 2023 - Today I Learned Artificial Intelligence (TIL-AI)\\Brainhack_2023\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\Downloads\\NUS\\Brainhack 2023 - Today I Learned Artificial Intelligence (TIL-AI)\\Brainhack_2023\\.venv\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:950\u001b[0m, in \u001b[0;36mGRU.forward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m    948\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcheck_forward_args(\u001b[39minput\u001b[39m, hx, batch_sizes)\n\u001b[0;32m    949\u001b[0m \u001b[39mif\u001b[39;00m batch_sizes \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 950\u001b[0m     result \u001b[39m=\u001b[39m _VF\u001b[39m.\u001b[39;49mgru(\u001b[39minput\u001b[39;49m, hx, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_flat_weights, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnum_layers,\n\u001b[0;32m    951\u001b[0m                      \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdropout, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbidirectional, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbatch_first)\n\u001b[0;32m    952\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    953\u001b[0m     result \u001b[39m=\u001b[39m _VF\u001b[39m.\u001b[39mgru(\u001b[39minput\u001b[39m, batch_sizes, hx, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_flat_weights, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias,\n\u001b[0;32m    954\u001b[0m                      \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_layers, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbidirectional)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "MANIFEST_FILE_TRAIN = './Train_Annotations.json'\n",
    "AUDIO_DIR_TRAIN = './audio_train/'\n",
    "SAVED_MODEL_PATH = './experimenting_model-5.pt'\n",
    "\n",
    "# # simple check on the saved model path, will raise error if no directory found\n",
    "# if not os.path.exists(os.path.dirname(SAVED_MODEL_PATH)):\n",
    "#     raise FileNotFoundError\n",
    "\n",
    "#Loads the dataset\n",
    "dataset = CustomSpeechDataset(\n",
    "    manifest_file=MANIFEST_FILE_TRAIN, \n",
    "    audio_dir=AUDIO_DIR_TRAIN, \n",
    "    is_test_set=False\n",
    ")\n",
    "\n",
    "#train_dev_split (80x20 split)\n",
    "train_proportion = int(0.8 * len(dataset))\n",
    "dataset_train = list(dataset)[:train_proportion]\n",
    "dataset_dev = list(dataset)[train_proportion:]\n",
    "\n",
    "\n",
    "hparams = {\n",
    "        \"n_cnn_layers\": 3,\n",
    "        \"n_rnn_layers\": 5,\n",
    "        \"rnn_dim\": 256,     # changed from 512 -> 256 (to improves the model's capacity to capture complex patterns)\n",
    "        \"n_class\": 28,      # 26 alphabets in caps + <SPACE> + blanks\n",
    "        \"n_feats\": 100,     # changed from 128 -> 100 (to remove the warning during model training)\n",
    "        \"stride\": 2,\n",
    "        \"dropout\": 0.1,\n",
    "        \"learning_rate\": 1e-4,\n",
    "        \"batch_size\": 8,\n",
    "        \"epochs\": 15\n",
    "    }\n",
    "\n",
    "start_time = time()\n",
    "\n",
    "#Start training the model\n",
    "main(\n",
    "    hparams=hparams, \n",
    "    train_dataset=dataset_train, \n",
    "    dev_dataset=dataset_dev, \n",
    "    saved_model_path=SAVED_MODEL_PATH\n",
    ")\n",
    "\n",
    "end_time = time()\n",
    "\n",
    "print(f\"Time taken for training: {(end_time-start_time)/(60*60)} hrs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer(hparams, test_dataset, model_path) -> Dict[str, str]:\n",
    "    \n",
    "    print('\\ngenerating inference ...')\n",
    "\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    torch.manual_seed(SEED)\n",
    "    \n",
    "    greedy_decoder = GreedyDecoder()\n",
    "    data_processor = DataProcessor()\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\n",
    "\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        dataset=test_dataset,\n",
    "        batch_size=16,\n",
    "        shuffle=False,\n",
    "        collate_fn=lambda x: data_processor.data_processing(x, 'test'),\n",
    "        **kwargs\n",
    "    )\n",
    "    \n",
    "    #Load the pretrained model\n",
    "    model = SpeechRecognitionModel(\n",
    "        hparams['n_cnn_layers'], \n",
    "        hparams['n_rnn_layers'], \n",
    "        hparams['rnn_dim'],\n",
    "        hparams['n_class'], \n",
    "        hparams['n_feats'], \n",
    "        hparams['stride'], \n",
    "        hparams['dropout']\n",
    "    ).to(device)\n",
    "    \n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model.eval()\n",
    "    output_dict = {}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, _data in tqdm(enumerate(test_loader)):\n",
    "            audio_path, spectrograms, input_lengths = _data\n",
    "            spectrograms = spectrograms.to(device)\n",
    "            output = model(spectrograms)  # (batch, time, n_class)\n",
    "            output = F.log_softmax(output, dim=2)\n",
    "            output = output.transpose(0, 1) # (time, batch, n_class) \n",
    "            decoded_preds_batch = greedy_decoder.decode(output.transpose(0, 1), labels=None, label_lengths=None, is_test=True)\n",
    "            \n",
    "            # batch prediction\n",
    "            for decoded_idx in range(len(decoded_preds_batch[0])):\n",
    "                output_dict[audio_path[decoded_idx]] = decoded_preds_batch[0][decoded_idx]\n",
    "                \n",
    "    print('done!\\n')\n",
    "    return output_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Same hyperparams as what is used to train the model\n",
    "hparams = {\n",
    "        \"n_cnn_layers\": 3,\n",
    "        \"n_rnn_layers\": 5,\n",
    "        \"rnn_dim\": 256,     # changed from 512 -> 256 (to improves the model's capacity to capture complex patterns)\n",
    "        \"n_class\": 28,      # 26 alphabets in caps + <SPACE> + blanks\n",
    "        \"n_feats\": 100,     # changed from 128 -> 100 (to remove the warning during model training)\n",
    "        \"stride\": 2,\n",
    "        \"dropout\": 0.1,\n",
    "        \"learning_rate\": 1e-4,\n",
    "        \"batch_size\": 8,\n",
    "        \"epochs\": 15\n",
    "    }\n",
    "\n",
    "SAVED_MODEL_PATH = './experimenting_model-5.pt'\n",
    "SUBMISSION_PATH = './final_results-5 (rnn_dim 256).csv'\n",
    "\n",
    "MANIFEST_FILE_TEST = './Test_Filenames.json'\n",
    "AUDIO_DIR_TEST = './audio_test/'\n",
    "\n",
    "dataset_test = CustomSpeechDataset(\n",
    "    manifest_file=MANIFEST_FILE_TEST, \n",
    "    audio_dir=AUDIO_DIR_TEST, \n",
    "    is_test_set=True)\n",
    "\n",
    "start_time = time()\n",
    "\n",
    "submission_dict = infer(\n",
    "    hparams=hparams, \n",
    "    test_dataset=dataset_test, \n",
    "    model_path=SAVED_MODEL_PATH\n",
    ")\n",
    "\n",
    "#Producing the final csv file for submission\n",
    "submission_list = []\n",
    "\n",
    "for key in submission_dict:\n",
    "    submission_list.append(\n",
    "        {\n",
    "            \"path\": os.path.basename(key),\n",
    "            \"annotation\": submission_dict[key]\n",
    "        }\n",
    "    )\n",
    "\n",
    "submission_df = pd.DataFrame(submission_list)\n",
    "submission_df.to_csv(SUBMISSION_PATH, index=False)\n",
    "\n",
    "end_time = time()\n",
    "\n",
    "print(f\"Time taken for inference: {(end_time-start_time)/60} min\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
