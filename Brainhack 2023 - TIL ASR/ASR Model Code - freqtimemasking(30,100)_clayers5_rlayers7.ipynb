{"nbformat":4,"nbformat_minor":5,"metadata":{"selected_hardware_size":"medium-gpu","noteable":{"last_transaction_id":"b29949e6-36d0-4845-aab2-c4d02b7246c0"},"kernel_info":{"name":"python3"},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"}},"cells":[{"id":"93358258-5bdd-457e-874b-2be0d8a61f67","cell_type":"markdown","source":"# ASR Advanced","metadata":{"noteable":{"cell_type":"markdown"}}},{"id":"6b812a31-5c52-4322-9bf3-66ae00f3fa8a","cell_type":"markdown","source":"## Check GPU Instance","metadata":{"noteable":{"cell_type":"markdown"}}},{"id":"b5e17636","cell_type":"code","metadata":{"noteable":{"output_collection_id":"8a941386-29d1-47f8-96cb-ae8f6a38391d"},"ExecuteTime":{"end_time":"2023-06-02T07:30:02.587672+00:00","start_time":"2023-06-02T07:30:00.829054+00:00"}},"execution_count":null,"source":"# check whether it is a GPU instance\nimport torch\ntorch.cuda.is_available()","outputs":[]},{"id":"d357290a-f792-4785-acbf-2266c4dbee26","cell_type":"markdown","source":"## Install Dependencies","metadata":{"noteable":{"cell_type":"markdown"}}},{"id":"c0b16021-a3e8-4bbe-934a-f1199391faaf","cell_type":"code","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"noteable":{"cell_type":"code","output_collection_id":"07f7f608-ad3d-4fdb-abbb-da2099e04475"},"ExecuteTime":{"end_time":"2023-06-02T07:30:35.200276+00:00","start_time":"2023-06-02T07:30:02.594280+00:00"},"scrolled":false},"execution_count":null,"source":"# installing dependencies, though some dependencies are built-in in colab, we want to make sure that the dependencies are the same for the student's environment as well\n!pip install tqdm==4.65.0\n!pip install jiwer==3.0.1   \n!pip install librosa==0.9.1\n!pip install pandas==2.0.0rc\n\n# # download specific version of torch and torchaudio (DEPENDENCY ISSUE WITH NOTEABLE)\n# !pip install torch==1.12.1+cu116 torchaudio==0.12.1 --extra-index-url https://download.pytorch.org/whl/cu116 ","outputs":[]},{"id":"58044091-1a2a-4de7-a975-1c1ed54ad675","cell_type":"code","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"noteable":{"cell_type":"code","output_collection_id":"84618881-e953-4610-b139-fc25b343c937"},"ExecuteTime":{"end_time":"2023-06-02T07:34:37.646408+00:00","start_time":"2023-06-02T07:34:35.609623+00:00"}},"execution_count":null,"source":"# check versioning of torch and pandas\n# check that torch is 2.0.1 and torchaudio is 2.0.2\n!pip list | grep -E 'torch|pandas'","outputs":[]},{"id":"c5cfb731-dc0e-4ef0-8c29-e671665f8ef0","cell_type":"code","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"noteable":{"cell_type":"code"}},"execution_count":null,"source":"","outputs":[]},{"id":"0c1313b9-5cd0-4c46-a3ca-59c6ebfed580","cell_type":"markdown","source":"## Boilerplate Code","metadata":{"noteable":{"cell_type":"markdown"}}},{"id":"8d7e2ed7-3d95-47e3-81e8-cad1e296dee6","cell_type":"markdown","source":"### Importing Libraries","metadata":{"noteable":{"cell_type":"markdown"}}},{"id":"2fbd6be1-874a-40e6-99fa-104cdade107a","cell_type":"code","metadata":{"noteable":{"cell_type":"python","output_collection_id":"cf9cb177-0547-4cb1-bfa3-bca9ddade2a3"},"ExecuteTime":{"end_time":"2023-06-02T07:34:41.484646+00:00","start_time":"2023-06-02T07:34:41.328633+00:00"}},"execution_count":null,"source":"# import necessary libraries\nimport os\nimport json\nfrom tqdm import tqdm\nfrom jiwer import wer, cer\nfrom time import time\nimport pandas as pd\nfrom typing import Tuple, Dict, List\n\nimport torch\nimport torch.nn.functional as F\nimport torchaudio\n\n# setting the random seed\nSEED = 2022","outputs":[]},{"id":"151310c8-b570-466b-8437-ccc707d1b0bd","cell_type":"code","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"noteable":{"cell_type":"code"}},"execution_count":null,"source":"","outputs":[]},{"id":"39c2f1df-430a-4738-a9a8-b52a5719eefc","cell_type":"markdown","source":"## Classes and Functions","metadata":{"noteable":{"cell_type":"markdown"}}},{"id":"1cd837fc-d043-4afb-a671-10186dcbf079","cell_type":"code","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"noteable":{"cell_type":"code","output_collection_id":"469db9b7-2d32-47e4-9d38-d196064089ef"},"ExecuteTime":{"end_time":"2023-06-02T07:34:48.189211+00:00","start_time":"2023-06-02T07:34:48.030027+00:00"}},"execution_count":null,"source":"class CustomSpeechDataset(torch.utils.data.Dataset):\n    \n    \"\"\"\n    Custom torch dataset class to load the dataset \n    \"\"\"\n    \n    def __init__(self, manifest_file: str, audio_dir: str, is_test_set: bool=False) -> None:\n\n        \"\"\"\n        manifest_file: the csv file that contains the filename of the audio, and also the annotation if is_test_set is set to False\n        audio_dir: the root directory of the audio datasets\n        is_test_set: the flag variable to switch between loading of the train and the test set. Train set loads the annotation whereas test set does not\n        \"\"\"\n\n        self.audio_dir = audio_dir\n        self.is_test_set = is_test_set\n\n        self.manifest = pd.read_csv(manifest_file)\n\n        \n    def __len__(self) -> int:\n        \n        \"\"\"\n        To get the number of loaded audio files in the dataset\n        \"\"\"\n\n        return len(self.manifest)\n    \n    \n    def __getitem__(self, index: int) -> Tuple[str, torch.Tensor]:\n\n        \"\"\"\n        To get the values required to do the training\n        \"\"\"\n\n        if torch.is_tensor(index):\n            index.tolist()\n            \n        audio_path = self._get_audio_path(index)\n        signal, sr = torchaudio.load(audio_path)\n        \n        if not self.is_test_set:\n            annotation = self._get_annotation(index)\n            return audio_path, signal, annotation\n        \n        return audio_path, signal\n    \n    \n    def _get_audio_path(self, index: int) -> str:\n\n        \"\"\"\n        Helper function to retrieve the audio path from the csv manifest file\n        \"\"\"\n        \n        path = os.path.join(self.audio_dir, self.manifest.iloc[index]['path'])\n\n        return path\n    \n    \n    def _get_annotation(self, index: int) -> str:\n\n        \"\"\"\n        Helper function to retrieve the annotation from the csv manifest file\n        \"\"\"\n\n        return self.manifest.iloc[index]['annotation']","outputs":[]},{"id":"e31fc68d-3dc7-40e4-9c36-38fb33e42d7d","cell_type":"code","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"noteable":{"cell_type":"code","output_collection_id":"ddfeff37-207b-4f4e-bdaf-6487abe0f76c"},"ExecuteTime":{"end_time":"2023-06-02T07:34:54.916196+00:00","start_time":"2023-06-02T07:34:54.757305+00:00"}},"execution_count":null,"source":"class TextTransform:\n\n    \"\"\"\n    Map characters to integers and vice versa (encoding/decoding)\n    \"\"\"\n    \n    def __init__(self) -> None:\n\n        char_map_str = \"\"\"\n            <SPACE> 0\n            A 1\n            B 2\n            C 3\n            D 4\n            E 5\n            F 6\n            G 7\n            H 8\n            I 9\n            J 10\n            K 11\n            L 12\n            M 13\n            N 14\n            O 15\n            P 16\n            Q 17\n            R 18\n            S 19\n            T 20\n            U 21\n            V 22\n            W 23\n            X 24\n            Y 25\n            Z 26\n        \"\"\"\n        \n        self.char_map = {}\n        self.index_map = {}\n        \n        for line in char_map_str.strip().split('\\n'):\n            ch, index = line.split()\n            self.char_map[ch] = int(index)\n            self.index_map[int(index)] = ch\n\n        self.index_map[0] = ' '\n\n\n    def get_char_len(self) -> int:\n\n        \"\"\"\n        Gets the number of characters that are being encoded and decoded in the prediction\n        Returns:\n        --------\n            the number of characters defined in the __init__ char_map_str\n        \"\"\"\n\n        return len(self.char_map)\n    \n\n    def get_char_list(self) -> List[str]:\n\n        \"\"\"\n        Gets the list of characters that are being encoded and decoded in the prediction\n        \n        Returns:\n        -------\n            a list of characters defined in the __init__ char_map_str\n        \"\"\"\n\n        return list(self.index_map.values())\n    \n\n    def text_to_int(self, text: str) -> List[int]:\n\n        \"\"\"\n        Use a character map and convert text to an integer sequence \n        Returns:\n        -------\n            a list of the text encoded to an integer sequence \n        \"\"\"\n        \n        int_sequence = []\n        for c in text:\n            if c == ' ':\n                ch = self.char_map['<SPACE>']\n            else:\n                ch = self.char_map[c]\n            int_sequence.append(ch)\n\n        return int_sequence\n    \n\n    def int_to_text(self, labels) -> str:\n\n        \"\"\"\n        Use a character map and convert integer labels to an text sequence \n        \n        Returns:\n        -------\n            the decoded transcription\n        \"\"\"\n        \n        string = []\n        for i in labels:\n            string.append(self.index_map[i])\n\n        return ''.join(string).replace('<SPACE>', ' ')","outputs":[]},{"id":"4b9d431e-531c-42a5-b347-0c7032548b1a","cell_type":"code","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"noteable":{"cell_type":"code","output_collection_id":"e06145b6-9df0-41ad-b642-3cf619666d13"},"ExecuteTime":{"end_time":"2023-06-02T07:35:04.487188+00:00","start_time":"2023-06-02T07:35:04.329147+00:00"}},"execution_count":null,"source":"class GreedyDecoder:\n\n    \"\"\"\n    Decodes the logits into characters to form the final transciption using the greedy decoding approach\n    \"\"\"\n\n    def __init__(self) -> None:\n        pass\n\n\n    def decode(\n            self, \n            output: torch.Tensor, \n            labels: torch.Tensor=None, \n            label_lengths: List[int]=None, \n            collapse_repeated: bool=True, \n            is_test: bool=False\n        ):\n        \n        \"\"\"\n        Main method to call for the decoding of the text from the predicted logits\n        \"\"\"\n        \n        text_transform = TextTransform()\n        arg_maxes = torch.argmax(output, dim=2)\n        decodes = []\n\n        # refer to char_map_str in the TextTransform class -> only have index from 0 to 26, hence 27 represents the case where the character is decoded as blank (NOT <SPACE>)\n        decoded_blank_idx = text_transform.get_char_len()\n\n        if not is_test:\n            targets = []\n\n        for i, args in enumerate(arg_maxes):\n            decode = []\n\n            if not is_test:\n                targets.append(text_transform.int_to_text(labels[i][:label_lengths[i]].tolist()))\n\n            for j, char_idx in enumerate(args):\n                if char_idx != decoded_blank_idx:\n                    if collapse_repeated and j != 0 and char_idx == args[j-1]:\n                        continue\n                    decode.append(char_idx.item())\n            decodes.append(text_transform.int_to_text(decode))\n\n        return decodes, targets if not is_test else decodes","outputs":[]},{"id":"1d45bb16-ced4-42f9-8e29-656c6689b8f9","cell_type":"code","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"noteable":{"cell_type":"code","output_collection_id":"1c1ec79d-ea02-47ae-85c7-fdc60be0504a"},"ExecuteTime":{"end_time":"2023-06-02T07:35:15.887914+00:00","start_time":"2023-06-02T07:35:15.726906+00:00"}},"execution_count":null,"source":"class DataProcessor:\n    \"\"\"\n    Transforms the audio waveform tensors into a melspectrogram\n    \"\"\"\n\n    def __init__(self) -> None:\n        pass\n    \n    \n    def _audio_transformation(self, is_train: bool=True):\n\n        return torch.nn.Sequential(\n                torchaudio.transforms.MelSpectrogram(sample_rate=16000, n_mels=128),\n                torchaudio.transforms.FrequencyMasking(freq_mask_param=30), #changeable\n                torchaudio.transforms.TimeMasking(time_mask_param=100)      #changeable\n            ) if is_train else torchaudio.transforms.MelSpectrogram(sample_rate=16000, n_mels=128)\n    \n\n    def data_processing(self, data, data_type='train'):\n\n        \"\"\"\n        Process the audio data to retrieve the spectrograms that will be used for the training\n        \"\"\"\n\n        text_transform = TextTransform()\n        spectrograms = []\n        input_lengths = []\n        audio_path_list = []\n\n        audio_transforms = self._audio_transformation(is_train=True) if data_type == 'train' else self._audio_transformation(is_train=False)\n\n        if data_type != 'test':  \n            labels = []\n            label_lengths = []\n\n            for audio_path, waveform, utterance in data:\n\n                spec = audio_transforms(waveform).squeeze(0).transpose(0, 1)\n                spectrograms.append(spec)\n                label = torch.Tensor(text_transform.text_to_int(utterance))\n                labels.append(label)\n                input_lengths.append(spec.shape[0]//2)\n                label_lengths.append(len(label))\n\n            spectrograms = torch.nn.utils.rnn.pad_sequence(spectrograms, batch_first=True).unsqueeze(1).transpose(2, 3)\n            labels = torch.nn.utils.rnn.pad_sequence(labels, batch_first=True)\n            return audio_path, spectrograms, labels, input_lengths, label_lengths\n\n        else:\n            for audio_path, waveform in data:\n\n                spec = audio_transforms(waveform).squeeze(0).transpose(0, 1)\n                spectrograms.append(spec)\n                input_lengths.append(spec.shape[0]//2)\n                audio_path_list.append(audio_path)\n\n            spectrograms = torch.nn.utils.rnn.pad_sequence(spectrograms, batch_first=True).unsqueeze(1).transpose(2, 3)\n            return audio_path_list, spectrograms, input_lengths","outputs":[]},{"id":"36a039ac-9cc1-4170-8f51-2421f8b80806","cell_type":"code","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"noteable":{"cell_type":"code","output_collection_id":"1e2ac3ba-0277-4b78-b7e3-c50c1f11b46b"},"ExecuteTime":{"end_time":"2023-06-02T07:35:21.598402+00:00","start_time":"2023-06-02T07:35:21.429918+00:00"}},"execution_count":null,"source":"\"\"\"\nbuilding the model with adaption of deepspeech2 -> https://arxiv.org/abs/1512.02595\n\ncode adapted from https://towardsdatascience.com/customer-case-study-building-an-end-to-end-speech-recognition-model-in-pytorch-with-assemblyai-473030e47c7c\n\"\"\"\n\nclass CNNLayerNorm(torch.nn.Module):\n    \n    \"\"\"\n    Layer normalization built for CNNs input\n    \"\"\"\n    \n    def __init__(self, n_feats: int) -> None:\n        super(CNNLayerNorm, self).__init__()\n\n        self.layer_norm = torch.nn.LayerNorm(n_feats)\n\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Input x of dimension -> (batch, channel, feature, time)\n        \"\"\"\n        \n        x = x.transpose(2, 3).contiguous() # (batch, channel, time, feature)\n        x = self.layer_norm(x)\n\n        return x.transpose(2, 3).contiguous() # (batch, channel, feature, time) \n\n\nclass ResidualCNN(torch.nn.Module):\n\n    \"\"\"\n    Residual CNN inspired by https://arxiv.org/pdf/1603.05027.pdf except with layer norm instead of batch norm\n    \"\"\"\n    \n    def __init__(self, in_channels: int, out_channels: int, kernel: int, stride: int, dropout: float, n_feats: int) -> None:\n        super(ResidualCNN, self).__init__()\n\n        self.cnn1 = torch.nn.Conv2d(in_channels, out_channels, kernel, stride, padding=kernel//2)\n        self.cnn2 = torch.nn.Conv2d(out_channels, out_channels, kernel, stride, padding=kernel//2)\n        self.dropout1 = torch.nn.Dropout(dropout)\n        self.dropout2 = torch.nn.Dropout(dropout)\n        self.layer_norm1 = CNNLayerNorm(n_feats)\n        self.layer_norm2 = CNNLayerNorm(n_feats)\n\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \n        \"\"\"\n        Model building for the Residual CNN layers\n        \n        Input x of dimension -> (batch, channel, feature, time)\n        \"\"\"\n\n        residual = x\n        x = self.layer_norm1(x)\n        x = F.gelu(x)\n        x = self.dropout1(x)\n        x = self.cnn1(x)\n        x = self.layer_norm2(x)\n        x = F.gelu(x)\n        x = self.dropout2(x)\n        x = self.cnn2(x)\n        x += residual\n\n        return x # (batch, channel, feature, time)\n\n\nclass BidirectionalGRU(torch.nn.Module):\n\n    \"\"\"\n    The Bidirectional GRU composite code block which will be used in the main SpeechRecognitionModel class\n    \"\"\"\n    \n    def __init__(self, rnn_dim: int, hidden_size: int, dropout: int, batch_first: int) -> None:\n        super(BidirectionalGRU, self).__init__()\n\n        self.BiGRU = torch.nn.GRU(\n            input_size=rnn_dim, \n            hidden_size=hidden_size,\n            num_layers=1, \n            batch_first=batch_first, \n            bidirectional=True\n        )\n        self.layer_norm = torch.nn.LayerNorm(rnn_dim)\n        self.dropout = torch.nn.Dropout(dropout)\n\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n\n        \"\"\"\n        Transformation of the layers in the Bidirectional GRU block\n        \"\"\"\n\n        x = self.layer_norm(x)\n        x = F.gelu(x)\n        x, _ = self.BiGRU(x)\n        x = self.dropout(x)\n\n        return x\n\n\nclass SpeechRecognitionModel(torch.nn.Module):\n\n    \"\"\"\n    The main ASR Model that the main code will interact with\n    \"\"\"\n    \n    def __init__(self, n_cnn_layers, n_rnn_layers, rnn_dim, n_class, n_feats, stride=2, dropout=0.1) -> None:\n        super(SpeechRecognitionModel, self).__init__()\n        \n        n_feats = n_feats//2\n        self.cnn = torch.nn.Conv2d(1, 32, 3, stride=stride, padding=3//2)  # cnn for extracting heirachal features\n\n        # n residual cnn layers with filter size of 32\n        self.rescnn_layers = torch.nn.Sequential(*[\n            ResidualCNN(\n                in_channels=32, \n                out_channels=32, \n                kernel=3, \n                stride=1, \n                dropout=dropout, \n                n_feats=n_feats\n            ) for _ in range(n_cnn_layers)\n        ])\n        self.fully_connected = torch.nn.Linear(n_feats*32, rnn_dim)\n        self.birnn_layers = torch.nn.Sequential(*[\n            BidirectionalGRU(\n                rnn_dim=rnn_dim if i==0 else rnn_dim*2,\n                hidden_size=rnn_dim, \n                dropout=dropout, \n                batch_first=i==0\n            ) for i in range(n_rnn_layers)\n        ])\n        self.classifier = torch.nn.Sequential(\n            torch.nn.Linear(rnn_dim*2, rnn_dim),  # birnn returns rnn_dim*2\n            torch.nn.GELU(),\n            torch.nn.Dropout(dropout),\n            torch.nn.Linear(rnn_dim, n_class)\n        )\n\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n\n        \"\"\"\n        Transformation of the layers in the ASR model block\n        \"\"\"\n\n        x = self.cnn(x)\n        x = self.rescnn_layers(x)\n        sizes = x.size()\n        x = x.view(sizes[0], sizes[1] * sizes[2], sizes[3])  # (batch, feature, time)\n        x = x.transpose(1, 2) # (batch, time, feature)\n        x = self.fully_connected(x)\n        x = self.birnn_layers(x)\n        x = self.classifier(x)\n        \n        return x","outputs":[]},{"id":"a6038ea0-3181-415f-b999-ac6d984e5893","cell_type":"code","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"noteable":{"cell_type":"code","output_collection_id":"49fb2148-97a2-4955-8f0b-eabbe9a08581"},"ExecuteTime":{"end_time":"2023-06-02T07:35:27.358271+00:00","start_time":"2023-06-02T07:35:27.195366+00:00"}},"execution_count":null,"source":"class IterMeter():\n\n    \"\"\"\n    Keeps track of the total iterations during the training and validation loop\n    \"\"\"\n    \n    def __init__(self) -> None:\n        self.val = 0\n\n\n    def step(self):\n        self.val += 1\n\n\n    def get(self):\n        return self.val\n    \n\nclass TrainingLoop:\n\n    \"\"\"\n    The main class to set up the training loop to train the model\n    \"\"\"\n\n    def __init__(self) -> None:\n        pass\n    \n\n    def train(self, model, device, train_loader, criterion, optimizer, scheduler, epoch, iter_meter) -> None:\n\n        \"\"\"\n        Training Loop\n        \"\"\"\n        \n        model.train()\n        data_len = len(train_loader.dataset)\n        \n        for batch_idx, _data in enumerate(train_loader):\n            audio_path, spectrograms, labels, input_lengths, label_lengths = _data \n            spectrograms, labels = spectrograms.to(device), labels.to(device)\n\n            optimizer.zero_grad()\n\n            output = model(spectrograms)  # (batch, time, n_class)\n            output = F.log_softmax(output, dim=2)\n            output = output.transpose(0, 1) # (time, batch, n_class)\n\n            loss = criterion(output, labels, input_lengths, label_lengths)\n            loss.backward()\n\n            optimizer.step()\n            iter_meter.step()\n            \n            if batch_idx % 100 == 0 or batch_idx == data_len:\n                print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n                    epoch, batch_idx * len(spectrograms), data_len,\n                    100. * batch_idx / len(train_loader), loss.item()))\n\n\n    def dev(self, model, device, dev_loader, criterion, scheduler, epoch, iter_meter) -> None:\n\n        \"\"\"\n        Validation Loop\n        \"\"\"\n        \n        print('\\nevaluating...')\n        model.eval()\n        val_loss = 0\n        test_cer, test_wer = [], []\n        greedy_decoder = GreedyDecoder()\n        \n        with torch.no_grad():\n            for i, _data in enumerate(dev_loader):\n                audio_path, spectrograms, labels, input_lengths, label_lengths = _data \n                spectrograms, labels = spectrograms.to(device), labels.to(device)\n\n                output = model(spectrograms)  # (batch, time, n_class)\n                output = F.log_softmax(output, dim=2)\n                output = output.transpose(0, 1) # (time, batch, n_class)\n\n                loss = criterion(output, labels, input_lengths, label_lengths)\n                val_loss += loss.item() / len(dev_loader)\n\n                decoded_preds, decoded_targets = greedy_decoder.decode(output.transpose(0, 1), labels=labels, label_lengths=label_lengths, is_test=False)\n                \n                # to compare the ground truth and the predicted annotation\n#                 print(decoded_preds)\n#                 print(decoded_targets)\n                \n                for j in range(len(decoded_preds)):\n                    test_cer.append(cer(decoded_targets[j], decoded_preds[j]))\n                    test_wer.append(wer(decoded_targets[j], decoded_preds[j]))\n\n        avg_cer = sum(test_cer)/len(test_cer)\n        avg_wer = sum(test_wer)/len(test_wer)\n        \n        scheduler.step(val_loss)\n\n        print('Dev set: Average loss: {:.4f}, Average CER: {:4f} Average WER: {:.4f}\\n'.format(val_loss, avg_cer, avg_wer))","outputs":[]},{"id":"9a0c8eff-ed62-4d75-b51f-b8fe413b7f10","cell_type":"code","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"noteable":{"cell_type":"code","output_collection_id":null},"ExecuteTime":{"end_time":"2023-05-28T04:03:31.623993+00:00","start_time":"2023-05-28T04:03:30.684407+00:00"}},"execution_count":null,"source":"","outputs":[]},{"id":"c54d40e3-64c2-4b59-a083-ca586088ea8e","cell_type":"markdown","source":"## Loading the Dataset to Noteable","metadata":{"noteable":{"cell_type":"markdown"}}},{"id":"fd7fc1f5-40fc-48f3-818a-2484b745f178","cell_type":"code","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"noteable":{"cell_type":"code","output_collection_id":"9b63b519-9978-495a-b8d3-983df5dad910"},"ExecuteTime":{"end_time":"2023-06-02T07:35:57.394677+00:00","start_time":"2023-06-02T07:35:27.364011+00:00"},"scrolled":false},"execution_count":null,"source":"# pull the dataset\n%ntbl pull datasets \"BrainHack TIL-23 Automatic Speech Recognition Advanced Hackathon\"","outputs":[]},{"id":"2ef8baa3-c08e-4d68-b314-2e575617e1bc","cell_type":"code","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"noteable":{"cell_type":"code","output_collection_id":"a446fa57-610e-42b6-9169-127e422afea2"},"ExecuteTime":{"end_time":"2023-06-02T07:36:07.905456+00:00","start_time":"2023-06-02T07:36:06.949027+00:00"}},"execution_count":null,"source":"!ls \"/etc/noteable/datasets/\"","outputs":[]},{"id":"a86d9a57-ccb2-4346-9cdf-56b7d4689066","cell_type":"code","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"noteable":{"cell_type":"code","output_collection_id":"5017620b-0e5c-4425-b3a5-87656cc6a746"},"ExecuteTime":{"end_time":"2023-06-02T07:36:08.913111+00:00","start_time":"2023-06-02T07:36:07.931391+00:00"}},"execution_count":null,"source":"# check if all the files has been pulled to the datasets directory\n!ls \"/etc/noteable/datasets/BrainHack TIL-23 Automatic Speech Recognition Advanced Hackathon/\"","outputs":[]},{"id":"467e31e8-4af9-4e7d-9f74-c0e70efec251","cell_type":"code","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"noteable":{"cell_type":"code","output_collection_id":"d87d6468-2042-434c-a09c-5e5efa51bfb3"},"ExecuteTime":{"end_time":"2023-06-02T07:36:33.841596+00:00","start_time":"2023-06-02T07:36:08.929900+00:00"},"scrolled":false},"execution_count":null,"source":"# unzip the dataset and place it into the /tmp/data directory\n!unzip \"/etc/noteable//datasets/BrainHack TIL-23 Automatic Speech Recognition Advanced Hackathon/Train.zip\" -d \"/tmp/data\"\n\n!unzip \"/etc/noteable//datasets/BrainHack TIL-23 Automatic Speech Recognition Advanced Hackathon/Test_Advanced.zip\" -d \"/tmp/data\"","outputs":[]},{"id":"09819479-8948-478c-8b8d-38ca9ccf5fb9","cell_type":"code","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"noteable":{"cell_type":"code","output_collection_id":"bc055130-0470-4306-a94a-5dde21dd4ff2"},"ExecuteTime":{"end_time":"2023-06-02T07:36:47.811132+00:00","start_time":"2023-06-02T07:36:46.859771+00:00"}},"execution_count":null,"source":"# check if this directory does not exists first\n!ls /tmp/data","outputs":[]},{"id":"8947f165-3b1e-4860-a66e-32fdfb45d37a","cell_type":"code","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"noteable":{"cell_type":"code","output_collection_id":"473fab76-8ef7-4825-9c70-1878a514f722"},"ExecuteTime":{"end_time":"2023-06-02T07:36:51.233521+00:00","start_time":"2023-06-02T07:36:47.829574+00:00"}},"execution_count":null,"source":"# copy the remaining files into the /tmp/data directory, making sure that all these .csv files are in the same directory as the datasets\n!cp \"/etc/noteable//datasets/BrainHack TIL-23 Automatic Speech Recognition Advanced Hackathon/SampleSubmission_Advanced.csv\" \"/tmp/data\"\n!cp \"/etc/noteable//datasets/BrainHack TIL-23 Automatic Speech Recognition Advanced Hackathon/Test_Advanced.csv\" \"/tmp/data\"\n!cp \"/etc/noteable//datasets/BrainHack TIL-23 Automatic Speech Recognition Advanced Hackathon/Train.csv\" \"/tmp/data\"\n!cp \"/etc/noteable//datasets/BrainHack TIL-23 Automatic Speech Recognition Advanced Hackathon/VariableDefinitions.csv\" \"/tmp/data\"","outputs":[]},{"id":"50b7d384-d985-4740-8ff3-5be62aa8670e","cell_type":"code","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"noteable":{"cell_type":"code","output_collection_id":"017179d4-1d28-4c49-8e99-f09ce0fe19ef"},"ExecuteTime":{"end_time":"2023-06-02T07:36:52.220856+00:00","start_time":"2023-06-02T07:36:51.242460+00:00"}},"execution_count":null,"source":"# check the datasets and the other files available\n!ls /tmp/data","outputs":[]},{"id":"de145c64-b6ba-4b74-82d8-afcf3d3a135f","cell_type":"code","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"noteable":{"cell_type":"code","output_collection_id":"544254d5-edee-49c9-9b21-e065b918f59e"},"ExecuteTime":{"end_time":"2023-06-01T19:57:25.885126+00:00","start_time":"2023-06-01T19:57:24.851720+00:00"},"scrolled":true},"execution_count":null,"source":"# check current working directory\n!pwd","outputs":[]},{"id":"193d7299-7c90-494b-97a7-37553576315a","cell_type":"code","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"noteable":{"cell_type":"code"}},"execution_count":null,"source":"","outputs":[]},{"id":"4ddc4d03-1b11-42eb-aa4d-64d44c513288","cell_type":"markdown","source":"## Training a Model","metadata":{"noteable":{"cell_type":"markdown"}}},{"id":"4720492d-ec62-4c94-a9a2-45ee423c712c","cell_type":"code","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"noteable":{"cell_type":"code","output_collection_id":"7e566c63-fa06-4d09-a218-f10d9cf28764"},"ExecuteTime":{"end_time":"2023-06-02T07:36:52.389745+00:00","start_time":"2023-06-02T07:36:52.229955+00:00"}},"execution_count":null,"source":"def main(hparams, train_dataset, dev_dataset, saved_model_path) -> None:\n\n    \"\"\"\n    The main method to call to do model training\n    \"\"\" \n\n    use_cuda = torch.cuda.is_available()\n    torch.manual_seed(SEED)\n    \n    data_processor = DataProcessor()\n    iter_meter = IterMeter()\n    text_transform = TextTransform()\n    trainer = TrainingLoop()\n    \n    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n    kwargs = {'num_workers': 4, 'pin_memory': True} if use_cuda else {}\n    \n    train_loader = torch.utils.data.DataLoader(\n        dataset=train_dataset,\n        batch_size=hparams['batch_size'],\n        shuffle=True,\n        collate_fn=lambda x: data_processor.data_processing(x, 'train'),\n        **kwargs\n    )\n    \n    dev_loader = torch.utils.data.DataLoader(\n        dataset=dev_dataset,\n        batch_size=hparams['batch_size'],\n        shuffle=False,\n        collate_fn=lambda x: data_processor.data_processing(x, 'dev'),\n        **kwargs\n    )\n\n    model = SpeechRecognitionModel(\n        hparams['n_cnn_layers'], \n        hparams['n_rnn_layers'], \n        hparams['rnn_dim'],\n        hparams['n_class'], \n        hparams['n_feats'], \n        hparams['stride'], \n        hparams['dropout']\n    ).to(device)\n\n    print('Num Model Parameters', sum([param.nelement() for param in model.parameters()]))\n\n    optimizer = torch.optim.AdamW(model.parameters(), hparams['learning_rate']) #changeable\n    criterion = torch.nn.CTCLoss(blank=text_transform.get_char_len()).to(device)\n    \n    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer, mode='min', patience=5, verbose=True, factor=0.001)\n    \n    for epoch in range(1, hparams['epochs'] + 1):\n        trainer.train(model, device, train_loader, criterion, optimizer, scheduler, epoch, iter_meter)\n        trainer.dev(model, device, dev_loader, criterion, scheduler, epoch, iter_meter)\n        \n    # save the trained model\n    torch.save(model.state_dict(), saved_model_path)\n","outputs":[]},{"id":"e52656e6-fdcb-47ff-a580-fb1d4ddba0f1","cell_type":"code","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"noteable":{"cell_type":"code"}},"execution_count":null,"source":"","outputs":[]},{"id":"1d24f1c9-68ac-4b37-9e4c-96d3c11e124c","cell_type":"markdown","source":"## Calling the functions and classes to train the model","metadata":{"noteable":{"cell_type":"markdown"}}},{"id":"1ffcb012-48e2-4a7f-9895-8c9fcbeda06b","cell_type":"code","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"noteable":{"cell_type":"code","output_collection_id":"3b8aba03-ade6-4cbd-a508-11b60bc7fb99"},"ExecuteTime":{"end_time":"2023-06-02T07:36:53.385186+00:00","start_time":"2023-06-02T07:36:52.427644+00:00"}},"execution_count":null,"source":"# final check on the dataset and the annotation \n!ls /tmp/data","outputs":[]},{"id":"99755fee-50ce-41d3-bf2d-1aa219f387a4","cell_type":"code","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"noteable":{"cell_type":"code","output_collection_id":"e9a6078e-a62b-4b38-9a58-e4aa36b8ff7d"},"ExecuteTime":{"end_time":"2023-06-02T08:43:41.720683+00:00","start_time":"2023-06-02T07:37:20.229295+00:00"},"scrolled":false},"execution_count":null,"source":"# state the filepath of the datasets and where the final model is saved\nMANIFEST_FILE_TRAIN = '/tmp/data/Train.csv'\nAUDIO_DIR_TRAIN = '/tmp/data/Train'\nSAVED_MODEL_PATH = '/tmp/data/model-freqtimemasking(30,100)_clayers5_rlayers7.pt'\n\n# simple check on the saved model path, will raise error if no directory found\nif not os.path.exists(os.path.dirname(SAVED_MODEL_PATH)):\n    raise FileNotFoundError\n\n# loads the dataset\ndataset = CustomSpeechDataset(\n    manifest_file=MANIFEST_FILE_TRAIN, \n    audio_dir=AUDIO_DIR_TRAIN, \n    is_test_set=False\n)\n\n# train_dev_split #changeable\ntrain_proportion = int(0.8 * len(dataset))\ndataset_train = list(dataset)[:train_proportion]\ndataset_dev = list(dataset)[train_proportion:]\n\n\nhparams = {\n        \"n_cnn_layers\": 5,      #changeable\n        \"n_rnn_layers\": 7,      #changeable\n        \"rnn_dim\": 512,         #changeable\n        \"n_class\": 28,          #26 alphabets in caps + <SPACE> + blanks\n        \"n_feats\": 128,\n        \"stride\": 2,            #changeable\n        \"dropout\": 0.1,         #changeable\n        \"learning_rate\": 5e-5,  #changeable\n        \"batch_size\": 8,        #changeable\n        \"epochs\": 50            #changeable\n  }\n\nstart_time = time()\n\n# start training the model\nmain(\n    hparams=hparams, \n    train_dataset=dataset_train, \n    dev_dataset=dataset_dev, \n    saved_model_path=SAVED_MODEL_PATH\n)\n\nend_time = time()\n\nprint(f\"Time taken for training: {(end_time-start_time)/(60*60)} hrs\")","outputs":[]},{"id":"66636f09-d198-4133-bb35-4323ac232f14","cell_type":"code","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"noteable":{"cell_type":"code","output_collection_id":"fb7285d6-3fce-477b-805a-fc69d37fc878"},"ExecuteTime":{"end_time":"2023-06-02T08:44:33.754310+00:00","start_time":"2023-06-02T08:44:32.628817+00:00"}},"execution_count":null,"source":"# check if model.pt is produced\n!ls /tmp/data","outputs":[]},{"id":"404db212-888b-4be5-b553-13a760250943","cell_type":"code","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"noteable":{"cell_type":"code","output_collection_id":"8fb4052b-192b-4eb4-aaca-7126cbac337b"},"ExecuteTime":{"end_time":"2023-06-02T08:45:09.889798+00:00","start_time":"2023-06-02T08:45:08.728793+00:00"}},"execution_count":null,"source":"# copy the model over to datasets\n!cp /tmp/data/'model-freqtimemasking(30,100)_clayers5_rlayers7.pt' /etc/noteable/project/'model-freqtimemasking(30,100)_clayers5_rlayers7.pt'","outputs":[]},{"id":"20c13507-f260-47e0-a8fa-bb725263e9d6","cell_type":"code","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"noteable":{"cell_type":"code","output_collection_id":"1e63014c-e06a-4510-9024-f445bfdebba3"},"ExecuteTime":{"end_time":"2023-06-02T08:45:19.483926+00:00","start_time":"2023-06-02T08:45:18.428278+00:00"},"scrolled":true},"execution_count":null,"source":"!ls /etc/noteable/project/","outputs":[]},{"id":"9f3c3db9-367e-4e55-bc02-5902e5513daa","cell_type":"code","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"noteable":{"cell_type":"code"}},"execution_count":null,"source":"","outputs":[]},{"id":"ab4910ab-12bb-4e6a-9fcc-61b41c928063","cell_type":"markdown","source":"## Inference","metadata":{"noteable":{"cell_type":"markdown"}}},{"id":"111400b4-72cc-463b-ac8b-889d0a71babf","cell_type":"code","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"noteable":{"cell_type":"code","output_collection_id":"74513786-12c8-4dbf-9c44-1962dda8bde7"},"ExecuteTime":{"end_time":"2023-06-02T08:45:59.196725+00:00","start_time":"2023-06-02T08:45:59.035320+00:00"}},"execution_count":null,"source":"def infer(hparams, test_dataset, model_path) -> Dict[str, str]:\n    \n    print('\\ngenerating inference ...')\n\n    use_cuda = torch.cuda.is_available()\n    torch.manual_seed(SEED)\n    \n    greedy_decoder = GreedyDecoder()\n    data_processor = DataProcessor()\n    \n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    kwargs = {'num_workers': 4, 'pin_memory': True} if use_cuda else {}\n\n    test_loader = torch.utils.data.DataLoader(\n        dataset=test_dataset,\n        batch_size=16,\n        shuffle=False,\n        collate_fn=lambda x: data_processor.data_processing(x, 'test'),\n        **kwargs\n    )\n    \n    # load the pretrained model\n    model = SpeechRecognitionModel(\n        hparams['n_cnn_layers'], \n        hparams['n_rnn_layers'], \n        hparams['rnn_dim'],\n        hparams['n_class'], \n        hparams['n_feats'], \n        hparams['stride'], \n        hparams['dropout']\n    ).to(device)\n    \n    model.load_state_dict(torch.load(model_path))\n    model.eval()\n    output_dict = {}\n    \n    with torch.no_grad():\n        for i, _data in tqdm(enumerate(test_loader)):\n            audio_path, spectrograms, input_lengths = _data\n            spectrograms = spectrograms.to(device)\n            output = model(spectrograms)  # (batch, time, n_class)\n            output = F.log_softmax(output, dim=2)\n            output = output.transpose(0, 1) # (time, batch, n_class) \n            decoded_preds_batch = greedy_decoder.decode(output.transpose(0, 1), labels=None, label_lengths=None, is_test=True)\n            \n            # batch prediction\n            for decoded_idx in range(len(decoded_preds_batch[0])):\n                output_dict[audio_path[decoded_idx]] = decoded_preds_batch[0][decoded_idx]\n                \n    print('done!\\n')\n    return output_dict","outputs":[]},{"id":"b7e8a29f-c25e-431a-b892-7a54e36da425","cell_type":"markdown","source":"## Calling the functions and classes to do model inference","metadata":{"noteable":{"cell_type":"markdown"}}},{"id":"ed010713-8bbb-48bc-97af-ad701a11ffdb","cell_type":"code","metadata":{"noteable":{"cell_type":"python","output_collection_id":"db2e31b8-962d-4f25-b0e4-b0fe598a0d16"},"ExecuteTime":{"end_time":"2023-06-02T08:46:10.556155+00:00","start_time":"2023-06-02T08:46:09.459421+00:00"}},"execution_count":null,"source":"!ls /tmp/data","outputs":[]},{"id":"4cd27206-d75e-4d8a-8b9e-6dcd932a076b","cell_type":"code","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"noteable":{"cell_type":"code","output_collection_id":"f4d98225-f2ad-4e81-8cec-ecb447943814"},"ExecuteTime":{"end_time":"2023-06-02T08:50:28.022132+00:00","start_time":"2023-06-02T08:47:33.929763+00:00"},"scrolled":false},"execution_count":null,"source":"# same hyperparams as what you have used to train the model\nhparams = {\n        \"n_cnn_layers\": 5,\n        \"n_rnn_layers\": 7,\n        \"rnn_dim\": 512,\n        \"n_class\": 28, # 26 alphabets in caps + <SPACE> + blanks\n        \"n_feats\": 128,\n        \"stride\": 2,\n        \"dropout\": 0.1,\n        \"learning_rate\": None, # None, because parameter not used for inference\n        \"batch_size\": None, # None, because parameter not used for inference\n        \"epochs\": None # None, because parameter not used for inference\n    }\n\n# change the filepath as according\nSAVED_MODEL_PATH = '/tmp/data/model-freqtimemasking(30,100)_clayers5_rlayers7.pt'\nSUBMISSION_PATH = '/tmp/data/Submission_Advanced-freqtimemasking(30,100)_clayers5_rlayers7.csv'\n\nMANIFEST_FILE_TEST = '/tmp/data/Test_Advanced.csv' \nAUDIO_DIR_TEST = '/tmp/data/Test_Advanced/'\n\ndataset_test = CustomSpeechDataset(\n    manifest_file=MANIFEST_FILE_TEST, \n    audio_dir=AUDIO_DIR_TEST, \n    is_test_set=True\n)\n\nstart_time = time()\n\nsubmission_dict = infer(\n    hparams=hparams, \n    test_dataset=dataset_test, \n    model_path=SAVED_MODEL_PATH\n)\n\n# producing the final csv file for submission\nsubmission_list = []\n\nfor key in submission_dict:\n    submission_list.append(\n        {\n            \"path\": os.path.basename(key),\n            \"annotation\": submission_dict[key]\n        }\n    )\n\nsubmission_df = pd.DataFrame(submission_list)\nsubmission_df.to_csv(SUBMISSION_PATH, index=False)\n\nend_time = time()\n\nprint(f\"Time taken for inference: {(end_time-start_time)/60} min\")","outputs":[]},{"id":"956ba0f1-a985-464b-acaa-6e6ab62c0437","cell_type":"code","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"noteable":{"cell_type":"code","output_collection_id":"29b806db-b46a-424f-8a14-d90bc945b670"},"ExecuteTime":{"end_time":"2023-06-02T08:50:44.848484+00:00","start_time":"2023-06-02T08:50:43.629814+00:00"}},"execution_count":null,"source":"# check if the Submission_Advanced.csv exists\n!ls /tmp/data/","outputs":[]},{"id":"6f8a339f-f74a-4533-aa1a-2ab6c1b3c21d","cell_type":"code","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"noteable":{"cell_type":"code","output_collection_id":"2fd9bd7e-8f76-470e-b6d5-0e815e559191"},"ExecuteTime":{"end_time":"2023-06-02T08:51:11.797039+00:00","start_time":"2023-06-02T08:51:10.628689+00:00"}},"execution_count":null,"source":"!cp /tmp/data/'Submission_Advanced-freqtimemasking(30,100)_clayers5_rlayers7.csv' /etc/noteable/project/","outputs":[]},{"id":"c8b3a47e-30ab-49d1-adfa-42eed8569521","cell_type":"code","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"noteable":{"cell_type":"code","output_collection_id":"51c5ab7b-7c88-418e-85ab-ce0be9dff073"},"ExecuteTime":{"end_time":"2023-06-02T08:51:24.387584+00:00","start_time":"2023-06-02T08:51:23.224134+00:00"},"scrolled":false},"execution_count":null,"source":"!ls /etc/noteable/project/","outputs":[]},{"id":"9dc7652c-a556-4e6e-a004-413cf09d540a","cell_type":"code","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"noteable":{"cell_type":"code"}},"execution_count":null,"source":"","outputs":[]},{"id":"2d0fc6ae-0def-4aee-95f2-ea1acca222a2","cell_type":"code","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"noteable":{"cell_type":"code"}},"execution_count":null,"source":"","outputs":[]}]}