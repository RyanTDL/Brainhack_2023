{"nbformat":4,"nbformat_minor":5,"metadata":{"selected_hardware_size":"medium-gpu","noteable":{"last_transaction_id":"263804d6-cf3d-48a1-899b-65b2f4f70967"},"kernel_info":{"name":"python3"},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"}},"cells":[{"id":"93358258-5bdd-457e-874b-2be0d8a61f67","cell_type":"markdown","source":"# ASR Advanced","metadata":{"noteable":{"cell_type":"markdown"}}},{"id":"6b812a31-5c52-4322-9bf3-66ae00f3fa8a","cell_type":"markdown","source":"## Check GPU Instance","metadata":{"noteable":{"cell_type":"markdown"}}},{"id":"b5e17636","cell_type":"code","metadata":{"noteable":{"output_collection_id":"481eb1cf-a0b2-4b6a-b698-04eaabfa6521"},"ExecuteTime":{"end_time":"2023-06-02T04:48:53.042804+00:00","start_time":"2023-06-02T04:48:52.886090+00:00"}},"execution_count":null,"source":"# check whether it is a GPU instance\nimport torch\ntorch.cuda.is_available()","outputs":[]},{"id":"d357290a-f792-4785-acbf-2266c4dbee26","cell_type":"markdown","source":"## Install Dependencies","metadata":{"noteable":{"cell_type":"markdown"}}},{"id":"c0b16021-a3e8-4bbe-934a-f1199391faaf","cell_type":"code","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"noteable":{"cell_type":"code","output_collection_id":"891c9e09-7ca1-43f0-806a-1dabf57184f1"},"ExecuteTime":{"end_time":"2023-06-02T04:49:11.906313+00:00","start_time":"2023-06-02T04:48:56.784356+00:00"},"scrolled":false},"execution_count":null,"source":"# installing dependencies, though some dependencies are built-in in colab, we want to make sure that the dependencies are the same for the student's environment as well\n!pip install tqdm==4.65.0\n!pip install jiwer==3.0.1   \n!pip install librosa==0.9.1\n!pip install pandas==2.0.0rc\n\n# # download specific version of torch and torchaudio (DEPENDENCY ISSUE WITH NOTEABLE)\n# !pip install torch==1.12.1+cu116 torchaudio==0.12.1 --extra-index-url https://download.pytorch.org/whl/cu116 ","outputs":[]},{"id":"58044091-1a2a-4de7-a975-1c1ed54ad675","cell_type":"code","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"noteable":{"cell_type":"code","output_collection_id":"c6415c6b-4a2a-420c-9073-1964576c8a85"},"ExecuteTime":{"end_time":"2023-06-02T04:49:13.889676+00:00","start_time":"2023-06-02T04:49:11.918761+00:00"}},"execution_count":null,"source":"# check versioning of torch and pandas\n# check that torch is 2.0.1 and torchaudio is 2.0.2\n!pip list | grep -E 'torch|pandas'","outputs":[]},{"id":"c5cfb731-dc0e-4ef0-8c29-e671665f8ef0","cell_type":"code","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"noteable":{"cell_type":"code"}},"execution_count":null,"source":"","outputs":[]},{"id":"0c1313b9-5cd0-4c46-a3ca-59c6ebfed580","cell_type":"markdown","source":"## Boilerplate Code","metadata":{"noteable":{"cell_type":"markdown"}}},{"id":"8d7e2ed7-3d95-47e3-81e8-cad1e296dee6","cell_type":"markdown","source":"### Importing Libraries","metadata":{"noteable":{"cell_type":"markdown"}}},{"id":"2fbd6be1-874a-40e6-99fa-104cdade107a","cell_type":"code","metadata":{"noteable":{"cell_type":"python","output_collection_id":"15f1d3d9-e4c6-48bc-934d-cf3dd2e90298"},"ExecuteTime":{"end_time":"2023-06-02T04:49:14.055786+00:00","start_time":"2023-06-02T04:49:13.896658+00:00"}},"execution_count":null,"source":"# import necessary libraries\nimport os\nimport json\nfrom tqdm import tqdm\nfrom jiwer import wer, cer\nfrom time import time\nimport pandas as pd\nfrom typing import Tuple, Dict, List\n\nimport torch\nimport torch.nn.functional as F\nimport torchaudio\n\n# setting the random seed\nSEED = 2022","outputs":[]},{"id":"151310c8-b570-466b-8437-ccc707d1b0bd","cell_type":"code","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"noteable":{"cell_type":"code"}},"execution_count":null,"source":"","outputs":[]},{"id":"39c2f1df-430a-4738-a9a8-b52a5719eefc","cell_type":"markdown","source":"## Classes and Functions","metadata":{"noteable":{"cell_type":"markdown"}}},{"id":"1cd837fc-d043-4afb-a671-10186dcbf079","cell_type":"code","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"noteable":{"cell_type":"code","output_collection_id":"001669bf-2dc2-47fc-bad9-1085f90e07bc"},"ExecuteTime":{"end_time":"2023-06-02T04:49:14.540378+00:00","start_time":"2023-06-02T04:49:14.381521+00:00"}},"execution_count":null,"source":"class CustomSpeechDataset(torch.utils.data.Dataset):\n    \n    \"\"\"\n    Custom torch dataset class to load the dataset \n    \"\"\"\n    \n    def __init__(self, manifest_file: str, audio_dir: str, is_test_set: bool=False) -> None:\n\n        \"\"\"\n        manifest_file: the csv file that contains the filename of the audio, and also the annotation if is_test_set is set to False\n        audio_dir: the root directory of the audio datasets\n        is_test_set: the flag variable to switch between loading of the train and the test set. Train set loads the annotation whereas test set does not\n        \"\"\"\n\n        self.audio_dir = audio_dir\n        self.is_test_set = is_test_set\n\n        self.manifest = pd.read_csv(manifest_file)\n\n        \n    def __len__(self) -> int:\n        \n        \"\"\"\n        To get the number of loaded audio files in the dataset\n        \"\"\"\n\n        return len(self.manifest)\n    \n    \n    def __getitem__(self, index: int) -> Tuple[str, torch.Tensor]:\n\n        \"\"\"\n        To get the values required to do the training\n        \"\"\"\n\n        if torch.is_tensor(index):\n            index.tolist()\n            \n        audio_path = self._get_audio_path(index)\n        signal, sr = torchaudio.load(audio_path)\n        \n        if not self.is_test_set:\n            annotation = self._get_annotation(index)\n            return audio_path, signal, annotation\n        \n        return audio_path, signal\n    \n    \n    def _get_audio_path(self, index: int) -> str:\n\n        \"\"\"\n        Helper function to retrieve the audio path from the csv manifest file\n        \"\"\"\n        \n        path = os.path.join(self.audio_dir, self.manifest.iloc[index]['path'])\n\n        return path\n    \n    \n    def _get_annotation(self, index: int) -> str:\n\n        \"\"\"\n        Helper function to retrieve the annotation from the csv manifest file\n        \"\"\"\n\n        return self.manifest.iloc[index]['annotation']","outputs":[]},{"id":"e31fc68d-3dc7-40e4-9c36-38fb33e42d7d","cell_type":"code","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"noteable":{"cell_type":"code","output_collection_id":"181d6453-88af-4c53-ad99-eb01dd7fa8bd"},"ExecuteTime":{"end_time":"2023-06-02T04:46:34.147375+00:00","start_time":"2023-06-02T04:46:31.267459+00:00"}},"execution_count":null,"source":"class TextTransform:\n\n    \"\"\"\n    Map characters to integers and vice versa (encoding/decoding)\n    \"\"\"\n    \n    def __init__(self) -> None:\n\n        char_map_str = \"\"\"\n            <SPACE> 0\n            A 1\n            B 2\n            C 3\n            D 4\n            E 5\n            F 6\n            G 7\n            H 8\n            I 9\n            J 10\n            K 11\n            L 12\n            M 13\n            N 14\n            O 15\n            P 16\n            Q 17\n            R 18\n            S 19\n            T 20\n            U 21\n            V 22\n            W 23\n            X 24\n            Y 25\n            Z 26\n        \"\"\"\n        \n        self.char_map = {}\n        self.index_map = {}\n        \n        for line in char_map_str.strip().split('\\n'):\n            ch, index = line.split()\n            self.char_map[ch] = int(index)\n            self.index_map[int(index)] = ch\n\n        self.index_map[0] = ' '\n\n\n    def get_char_len(self) -> int:\n\n        \"\"\"\n        Gets the number of characters that are being encoded and decoded in the prediction\n        Returns:\n        --------\n            the number of characters defined in the __init__ char_map_str\n        \"\"\"\n\n        return len(self.char_map)\n    \n\n    def get_char_list(self) -> List[str]:\n\n        \"\"\"\n        Gets the list of characters that are being encoded and decoded in the prediction\n        \n        Returns:\n        -------\n            a list of characters defined in the __init__ char_map_str\n        \"\"\"\n\n        return list(self.index_map.values())\n    \n\n    def text_to_int(self, text: str) -> List[int]:\n\n        \"\"\"\n        Use a character map and convert text to an integer sequence \n        Returns:\n        -------\n            a list of the text encoded to an integer sequence \n        \"\"\"\n        \n        int_sequence = []\n        for c in text:\n            if c == ' ':\n                ch = self.char_map['<SPACE>']\n            else:\n                ch = self.char_map[c]\n            int_sequence.append(ch)\n\n        return int_sequence\n    \n\n    def int_to_text(self, labels) -> str:\n\n        \"\"\"\n        Use a character map and convert integer labels to an text sequence \n        \n        Returns:\n        -------\n            the decoded transcription\n        \"\"\"\n        \n        string = []\n        for i in labels:\n            string.append(self.index_map[i])\n\n        return ''.join(string).replace('<SPACE>', ' ')","outputs":[]},{"id":"4b9d431e-531c-42a5-b347-0c7032548b1a","cell_type":"code","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"noteable":{"cell_type":"code","output_collection_id":"68194834-5a79-4cf9-8937-b24c00c22d39"},"ExecuteTime":{"end_time":"2023-06-02T04:49:20.847179+00:00","start_time":"2023-06-02T04:49:20.689100+00:00"}},"execution_count":null,"source":"class GreedyDecoder:\n\n    \"\"\"\n    Decodes the logits into characters to form the final transciption using the greedy decoding approach\n    \"\"\"\n\n    def __init__(self) -> None:\n        pass\n\n\n    def decode(\n            self, \n            output: torch.Tensor, \n            labels: torch.Tensor=None, \n            label_lengths: List[int]=None, \n            collapse_repeated: bool=True, \n            is_test: bool=False\n        ):\n        \n        \"\"\"\n        Main method to call for the decoding of the text from the predicted logits\n        \"\"\"\n        \n        text_transform = TextTransform()\n        arg_maxes = torch.argmax(output, dim=2)\n        decodes = []\n\n        # refer to char_map_str in the TextTransform class -> only have index from 0 to 26, hence 27 represents the case where the character is decoded as blank (NOT <SPACE>)\n        decoded_blank_idx = text_transform.get_char_len()\n\n        if not is_test:\n            targets = []\n\n        for i, args in enumerate(arg_maxes):\n            decode = []\n\n            if not is_test:\n                targets.append(text_transform.int_to_text(labels[i][:label_lengths[i]].tolist()))\n\n            for j, char_idx in enumerate(args):\n                if char_idx != decoded_blank_idx:\n                    if collapse_repeated and j != 0 and char_idx == args[j-1]:\n                        continue\n                    decode.append(char_idx.item())\n            decodes.append(text_transform.int_to_text(decode))\n\n        return decodes, targets if not is_test else decodes","outputs":[]},{"id":"1d45bb16-ced4-42f9-8e29-656c6689b8f9","cell_type":"code","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"noteable":{"cell_type":"code","output_collection_id":"2315283c-4b21-4630-bf39-be02b760c8e1"},"ExecuteTime":{"end_time":"2023-06-02T04:49:26.007539+00:00","start_time":"2023-06-02T04:49:25.846254+00:00"}},"execution_count":null,"source":"class DataProcessor:\n    \"\"\"\n    Transforms the audio waveform tensors into a melspectrogram\n    \"\"\"\n\n    def __init__(self) -> None:\n        pass\n    \n    \n    def _audio_transformation(self, is_train: bool=True):\n\n        return torch.nn.Sequential(\n                torchaudio.transforms.MelSpectrogram(sample_rate=16000, n_mels=128),\n                torchaudio.transforms.FrequencyMasking(freq_mask_param=50), #changeable\n                torchaudio.transforms.TimeMasking(time_mask_param=120)      #changeable\n            ) if is_train else torchaudio.transforms.MelSpectrogram(sample_rate=16000, n_mels=128)\n    \n\n    def data_processing(self, data, data_type='train'):\n\n        \"\"\"\n        Process the audio data to retrieve the spectrograms that will be used for the training\n        \"\"\"\n\n        text_transform = TextTransform()\n        spectrograms = []\n        input_lengths = []\n        audio_path_list = []\n\n        audio_transforms = self._audio_transformation(is_train=True) if data_type == 'train' else self._audio_transformation(is_train=False)\n\n        if data_type != 'test':  \n            labels = []\n            label_lengths = []\n\n            for audio_path, waveform, utterance in data:\n\n                spec = audio_transforms(waveform).squeeze(0).transpose(0, 1)\n                spectrograms.append(spec)\n                label = torch.Tensor(text_transform.text_to_int(utterance))\n                labels.append(label)\n                input_lengths.append(spec.shape[0]//2)\n                label_lengths.append(len(label))\n\n            spectrograms = torch.nn.utils.rnn.pad_sequence(spectrograms, batch_first=True).unsqueeze(1).transpose(2, 3)\n            labels = torch.nn.utils.rnn.pad_sequence(labels, batch_first=True)\n            return audio_path, spectrograms, labels, input_lengths, label_lengths\n\n        else:\n            for audio_path, waveform in data:\n\n                spec = audio_transforms(waveform).squeeze(0).transpose(0, 1)\n                spectrograms.append(spec)\n                input_lengths.append(spec.shape[0]//2)\n                audio_path_list.append(audio_path)\n\n            spectrograms = torch.nn.utils.rnn.pad_sequence(spectrograms, batch_first=True).unsqueeze(1).transpose(2, 3)\n            return audio_path_list, spectrograms, input_lengths","outputs":[]},{"id":"36a039ac-9cc1-4170-8f51-2421f8b80806","cell_type":"code","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"noteable":{"cell_type":"code","output_collection_id":"ac59d8d2-68e1-4e20-baad-13675afe7cc2"},"ExecuteTime":{"end_time":"2023-06-02T04:49:33.878920+00:00","start_time":"2023-06-02T04:49:33.705924+00:00"}},"execution_count":null,"source":"\"\"\"\nbuilding the model with adaption of deepspeech2 -> https://arxiv.org/abs/1512.02595\n\ncode adapted from https://towardsdatascience.com/customer-case-study-building-an-end-to-end-speech-recognition-model-in-pytorch-with-assemblyai-473030e47c7c\n\"\"\"\n\nclass CNNLayerNorm(torch.nn.Module):\n    \n    \"\"\"\n    Layer normalization built for CNNs input\n    \"\"\"\n    \n    def __init__(self, n_feats: int) -> None:\n        super(CNNLayerNorm, self).__init__()\n\n        self.layer_norm = torch.nn.LayerNorm(n_feats)\n\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Input x of dimension -> (batch, channel, feature, time)\n        \"\"\"\n        \n        x = x.transpose(2, 3).contiguous() # (batch, channel, time, feature)\n        x = self.layer_norm(x)\n\n        return x.transpose(2, 3).contiguous() # (batch, channel, feature, time) \n\n\nclass ResidualCNN(torch.nn.Module):\n\n    \"\"\"\n    Residual CNN inspired by https://arxiv.org/pdf/1603.05027.pdf except with layer norm instead of batch norm\n    \"\"\"\n    \n    def __init__(self, in_channels: int, out_channels: int, kernel: int, stride: int, dropout: float, n_feats: int) -> None:\n        super(ResidualCNN, self).__init__()\n\n        self.cnn1 = torch.nn.Conv2d(in_channels, out_channels, kernel, stride, padding=kernel//2)\n        self.cnn2 = torch.nn.Conv2d(out_channels, out_channels, kernel, stride, padding=kernel//2)\n        self.dropout1 = torch.nn.Dropout(dropout)\n        self.dropout2 = torch.nn.Dropout(dropout)\n        self.layer_norm1 = CNNLayerNorm(n_feats)\n        self.layer_norm2 = CNNLayerNorm(n_feats)\n\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \n        \"\"\"\n        Model building for the Residual CNN layers\n        \n        Input x of dimension -> (batch, channel, feature, time)\n        \"\"\"\n\n        residual = x\n        x = self.layer_norm1(x)\n        x = F.gelu(x)\n        x = self.dropout1(x)\n        x = self.cnn1(x)\n        x = self.layer_norm2(x)\n        x = F.gelu(x)\n        x = self.dropout2(x)\n        x = self.cnn2(x)\n        x += residual\n\n        return x # (batch, channel, feature, time)\n\n\nclass BidirectionalGRU(torch.nn.Module):\n\n    \"\"\"\n    The Bidirectional GRU composite code block which will be used in the main SpeechRecognitionModel class\n    \"\"\"\n    \n    def __init__(self, rnn_dim: int, hidden_size: int, dropout: int, batch_first: int) -> None:\n        super(BidirectionalGRU, self).__init__()\n\n        self.BiGRU = torch.nn.GRU(\n            input_size=rnn_dim, \n            hidden_size=hidden_size,\n            num_layers=1, \n            batch_first=batch_first, \n            bidirectional=True\n        )\n        self.layer_norm = torch.nn.LayerNorm(rnn_dim)\n        self.dropout = torch.nn.Dropout(dropout)\n\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n\n        \"\"\"\n        Transformation of the layers in the Bidirectional GRU block\n        \"\"\"\n\n        x = self.layer_norm(x)\n        x = F.gelu(x)\n        x, _ = self.BiGRU(x)\n        x = self.dropout(x)\n\n        return x\n\n\nclass SpeechRecognitionModel(torch.nn.Module):\n\n    \"\"\"\n    The main ASR Model that the main code will interact with\n    \"\"\"\n    \n    def __init__(self, n_cnn_layers, n_rnn_layers, rnn_dim, n_class, n_feats, stride=2, dropout=0.1) -> None:\n        super(SpeechRecognitionModel, self).__init__()\n        \n        n_feats = n_feats//2\n        self.cnn = torch.nn.Conv2d(1, 32, 3, stride=stride, padding=3//2)  # cnn for extracting heirachal features\n\n        # n residual cnn layers with filter size of 32\n        self.rescnn_layers = torch.nn.Sequential(*[\n            ResidualCNN(\n                in_channels=32, \n                out_channels=32, \n                kernel=3, \n                stride=1, \n                dropout=dropout, \n                n_feats=n_feats\n            ) for _ in range(n_cnn_layers)\n        ])\n        self.fully_connected = torch.nn.Linear(n_feats*32, rnn_dim)\n        self.birnn_layers = torch.nn.Sequential(*[\n            BidirectionalGRU(\n                rnn_dim=rnn_dim if i==0 else rnn_dim*2,\n                hidden_size=rnn_dim, \n                dropout=dropout, \n                batch_first=i==0\n            ) for i in range(n_rnn_layers)\n        ])\n        self.classifier = torch.nn.Sequential(\n            torch.nn.Linear(rnn_dim*2, rnn_dim),  # birnn returns rnn_dim*2\n            torch.nn.GELU(),\n            torch.nn.Dropout(dropout),\n            torch.nn.Linear(rnn_dim, n_class)\n        )\n\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n\n        \"\"\"\n        Transformation of the layers in the ASR model block\n        \"\"\"\n\n        x = self.cnn(x)\n        x = self.rescnn_layers(x)\n        sizes = x.size()\n        x = x.view(sizes[0], sizes[1] * sizes[2], sizes[3])  # (batch, feature, time)\n        x = x.transpose(1, 2) # (batch, time, feature)\n        x = self.fully_connected(x)\n        x = self.birnn_layers(x)\n        x = self.classifier(x)\n        \n        return x","outputs":[]},{"id":"a6038ea0-3181-415f-b999-ac6d984e5893","cell_type":"code","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"noteable":{"cell_type":"code","output_collection_id":"817f18fd-2d57-452f-94e5-0ac159147f71"},"ExecuteTime":{"end_time":"2023-06-02T04:49:38.247339+00:00","start_time":"2023-06-02T04:49:38.081621+00:00"}},"execution_count":null,"source":"class IterMeter():\n\n    \"\"\"\n    Keeps track of the total iterations during the training and validation loop\n    \"\"\"\n    \n    def __init__(self) -> None:\n        self.val = 0\n\n\n    def step(self):\n        self.val += 1\n\n\n    def get(self):\n        return self.val\n    \n\nclass TrainingLoop:\n\n    \"\"\"\n    The main class to set up the training loop to train the model\n    \"\"\"\n\n    def __init__(self) -> None:\n        pass\n    \n\n    def train(self, model, device, train_loader, criterion, optimizer, scheduler, epoch, iter_meter) -> None:\n\n        \"\"\"\n        Training Loop\n        \"\"\"\n        \n        model.train()\n        data_len = len(train_loader.dataset)\n        \n        for batch_idx, _data in enumerate(train_loader):\n            audio_path, spectrograms, labels, input_lengths, label_lengths = _data \n            spectrograms, labels = spectrograms.to(device), labels.to(device)\n\n            optimizer.zero_grad()\n\n            output = model(spectrograms)  # (batch, time, n_class)\n            output = F.log_softmax(output, dim=2)\n            output = output.transpose(0, 1) # (time, batch, n_class)\n\n            loss = criterion(output, labels, input_lengths, label_lengths)\n            loss.backward()\n\n            optimizer.step()\n            iter_meter.step()\n            \n            if batch_idx % 100 == 0 or batch_idx == data_len:\n                print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n                    epoch, batch_idx * len(spectrograms), data_len,\n                    100. * batch_idx / len(train_loader), loss.item()))\n\n\n    def dev(self, model, device, dev_loader, criterion, scheduler, epoch, iter_meter) -> None:\n\n        \"\"\"\n        Validation Loop\n        \"\"\"\n        \n        print('\\nevaluating...')\n        model.eval()\n        val_loss = 0\n        test_cer, test_wer = [], []\n        greedy_decoder = GreedyDecoder()\n        \n        with torch.no_grad():\n            for i, _data in enumerate(dev_loader):\n                audio_path, spectrograms, labels, input_lengths, label_lengths = _data \n                spectrograms, labels = spectrograms.to(device), labels.to(device)\n\n                output = model(spectrograms)  # (batch, time, n_class)\n                output = F.log_softmax(output, dim=2)\n                output = output.transpose(0, 1) # (time, batch, n_class)\n\n                loss = criterion(output, labels, input_lengths, label_lengths)\n                val_loss += loss.item() / len(dev_loader)\n\n                decoded_preds, decoded_targets = greedy_decoder.decode(output.transpose(0, 1), labels=labels, label_lengths=label_lengths, is_test=False)\n                \n                # to compare the ground truth and the predicted annotation\n#                 print(decoded_preds)\n#                 print(decoded_targets)\n                \n                for j in range(len(decoded_preds)):\n                    test_cer.append(cer(decoded_targets[j], decoded_preds[j]))\n                    test_wer.append(wer(decoded_targets[j], decoded_preds[j]))\n\n        avg_cer = sum(test_cer)/len(test_cer)\n        avg_wer = sum(test_wer)/len(test_wer)\n        \n        scheduler.step(val_loss)\n\n        print('Dev set: Average loss: {:.4f}, Average CER: {:4f} Average WER: {:.4f}\\n'.format(val_loss, avg_cer, avg_wer))","outputs":[]},{"id":"9a0c8eff-ed62-4d75-b51f-b8fe413b7f10","cell_type":"code","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"noteable":{"cell_type":"code","output_collection_id":null},"ExecuteTime":{"end_time":"2023-05-28T04:03:31.623993+00:00","start_time":"2023-05-28T04:03:30.684407+00:00"}},"execution_count":null,"source":"","outputs":[]},{"id":"c54d40e3-64c2-4b59-a083-ca586088ea8e","cell_type":"markdown","source":"## Loading the Dataset to Noteable","metadata":{"noteable":{"cell_type":"markdown"}}},{"id":"fd7fc1f5-40fc-48f3-818a-2484b745f178","cell_type":"code","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"noteable":{"cell_type":"code","output_collection_id":"7ae2a543-1045-45ab-9645-69a950ec3d9f"},"ExecuteTime":{"end_time":"2023-06-02T04:50:38.567842+00:00","start_time":"2023-06-02T04:50:07.522561+00:00"},"scrolled":false},"execution_count":null,"source":"# pull the dataset\n%ntbl pull datasets \"BrainHack TIL-23 Automatic Speech Recognition Advanced Hackathon\"","outputs":[]},{"id":"2ef8baa3-c08e-4d68-b314-2e575617e1bc","cell_type":"code","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"noteable":{"cell_type":"code","output_collection_id":"e84ddf0f-3985-45af-9ced-e59d1117c2ae"},"ExecuteTime":{"end_time":"2023-06-02T04:51:05.695780+00:00","start_time":"2023-06-02T04:51:04.679908+00:00"}},"execution_count":null,"source":"!ls \"/etc/noteable/datasets/\"","outputs":[]},{"id":"a86d9a57-ccb2-4346-9cdf-56b7d4689066","cell_type":"code","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"noteable":{"cell_type":"code","output_collection_id":"613a7b47-00dc-4953-8368-ee9d059484dc"},"ExecuteTime":{"end_time":"2023-06-02T04:51:11.442500+00:00","start_time":"2023-06-02T04:51:10.480617+00:00"}},"execution_count":null,"source":"# check if all the files has been pulled to the datasets directory\n!ls \"/etc/noteable/datasets/BrainHack TIL-23 Automatic Speech Recognition Advanced Hackathon/\"","outputs":[]},{"id":"467e31e8-4af9-4e7d-9f74-c0e70efec251","cell_type":"code","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"noteable":{"cell_type":"code","output_collection_id":"b497f82f-dda4-42d4-ae1d-60df8e28e133"},"ExecuteTime":{"end_time":"2023-06-02T04:51:41.768011+00:00","start_time":"2023-06-02T04:51:16.479644+00:00"},"scrolled":false},"execution_count":null,"source":"# unzip the dataset and place it into the /tmp/data directory\n!unzip \"/etc/noteable//datasets/BrainHack TIL-23 Automatic Speech Recognition Advanced Hackathon/Train.zip\" -d \"/tmp/data\"\n\n!unzip \"/etc/noteable//datasets/BrainHack TIL-23 Automatic Speech Recognition Advanced Hackathon/Test_Advanced.zip\" -d \"/tmp/data\"","outputs":[]},{"id":"09819479-8948-478c-8b8d-38ca9ccf5fb9","cell_type":"code","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"noteable":{"cell_type":"code","output_collection_id":"38ca4907-55b8-478e-a0a1-ee8afb0db9ba"},"ExecuteTime":{"end_time":"2023-06-02T04:52:11.565911+00:00","start_time":"2023-06-02T04:52:10.589422+00:00"}},"execution_count":null,"source":"# check if this directory does not exists first\n!ls /tmp/data","outputs":[]},{"id":"8947f165-3b1e-4860-a66e-32fdfb45d37a","cell_type":"code","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"noteable":{"cell_type":"code","output_collection_id":"93d32ea7-56bb-407e-b3a7-8d82366a172b"},"ExecuteTime":{"end_time":"2023-06-02T04:52:20.299638+00:00","start_time":"2023-06-02T04:52:16.881421+00:00"}},"execution_count":null,"source":"# copy the remaining files into the /tmp/data directory, making sure that all these .csv files are in the same directory as the datasets\n!cp \"/etc/noteable//datasets/BrainHack TIL-23 Automatic Speech Recognition Advanced Hackathon/SampleSubmission_Advanced.csv\" \"/tmp/data\"\n!cp \"/etc/noteable//datasets/BrainHack TIL-23 Automatic Speech Recognition Advanced Hackathon/Test_Advanced.csv\" \"/tmp/data\"\n!cp \"/etc/noteable//datasets/BrainHack TIL-23 Automatic Speech Recognition Advanced Hackathon/Train.csv\" \"/tmp/data\"\n!cp \"/etc/noteable//datasets/BrainHack TIL-23 Automatic Speech Recognition Advanced Hackathon/VariableDefinitions.csv\" \"/tmp/data\"","outputs":[]},{"id":"50b7d384-d985-4740-8ff3-5be62aa8670e","cell_type":"code","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"noteable":{"cell_type":"code","output_collection_id":"5c57efb5-5525-4d6a-a107-d812a3d635e9"},"ExecuteTime":{"end_time":"2023-06-02T04:52:25.243918+00:00","start_time":"2023-06-02T04:52:24.216568+00:00"}},"execution_count":null,"source":"# check the datasets and the other files available\n!ls /tmp/data","outputs":[]},{"id":"de145c64-b6ba-4b74-82d8-afcf3d3a135f","cell_type":"code","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"noteable":{"cell_type":"code","output_collection_id":"0da76093-bbe4-4584-9c89-620294785486"},"ExecuteTime":{"end_time":"2023-06-01T19:57:25.885126+00:00","start_time":"2023-06-01T19:57:24.851720+00:00"},"scrolled":true},"execution_count":null,"source":"# check current working directory\n!pwd","outputs":[]},{"id":"193d7299-7c90-494b-97a7-37553576315a","cell_type":"code","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"noteable":{"cell_type":"code"}},"execution_count":null,"source":"","outputs":[]},{"id":"4ddc4d03-1b11-42eb-aa4d-64d44c513288","cell_type":"markdown","source":"## Training a Model","metadata":{"noteable":{"cell_type":"markdown"}}},{"id":"4720492d-ec62-4c94-a9a2-45ee423c712c","cell_type":"code","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"noteable":{"cell_type":"code","output_collection_id":"615867eb-3685-472f-9a52-21490c7b8cbb"},"ExecuteTime":{"end_time":"2023-06-02T04:52:37.141282+00:00","start_time":"2023-06-02T04:52:36.980002+00:00"}},"execution_count":null,"source":"def main(hparams, train_dataset, dev_dataset, saved_model_path) -> None:\n\n    \"\"\"\n    The main method to call to do model training\n    \"\"\" \n\n    use_cuda = torch.cuda.is_available()\n    torch.manual_seed(SEED)\n    \n    data_processor = DataProcessor()\n    iter_meter = IterMeter()\n    text_transform = TextTransform()\n    trainer = TrainingLoop()\n    \n    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n    kwargs = {'num_workers': 4, 'pin_memory': True} if use_cuda else {}\n    \n    train_loader = torch.utils.data.DataLoader(\n        dataset=train_dataset,\n        batch_size=hparams['batch_size'],\n        shuffle=True,\n        collate_fn=lambda x: data_processor.data_processing(x, 'train'),\n        **kwargs\n    )\n    \n    dev_loader = torch.utils.data.DataLoader(\n        dataset=dev_dataset,\n        batch_size=hparams['batch_size'],\n        shuffle=False,\n        collate_fn=lambda x: data_processor.data_processing(x, 'dev'),\n        **kwargs\n    )\n\n    model = SpeechRecognitionModel(\n        hparams['n_cnn_layers'], \n        hparams['n_rnn_layers'], \n        hparams['rnn_dim'],\n        hparams['n_class'], \n        hparams['n_feats'], \n        hparams['stride'], \n        hparams['dropout']\n    ).to(device)\n\n    print('Num Model Parameters', sum([param.nelement() for param in model.parameters()]))\n\n    optimizer = torch.optim.AdamW(model.parameters(), hparams['learning_rate']) #changeable\n    criterion = torch.nn.CTCLoss(blank=text_transform.get_char_len()).to(device)\n    \n    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer, mode='min', patience=5, verbose=True, factor=0.001)\n    \n    for epoch in range(1, hparams['epochs'] + 1):\n        trainer.train(model, device, train_loader, criterion, optimizer, scheduler, epoch, iter_meter)\n        trainer.dev(model, device, dev_loader, criterion, scheduler, epoch, iter_meter)\n        \n    # save the trained model\n    torch.save(model.state_dict(), saved_model_path)\n","outputs":[]},{"id":"e52656e6-fdcb-47ff-a580-fb1d4ddba0f1","cell_type":"code","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"noteable":{"cell_type":"code"}},"execution_count":null,"source":"","outputs":[]},{"id":"1d24f1c9-68ac-4b37-9e4c-96d3c11e124c","cell_type":"markdown","source":"## Calling the functions and classes to train the model","metadata":{"noteable":{"cell_type":"markdown"}}},{"id":"1ffcb012-48e2-4a7f-9895-8c9fcbeda06b","cell_type":"code","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"noteable":{"cell_type":"code","output_collection_id":"08ed6f61-7d39-447a-b243-b975dbfcb9d7"},"ExecuteTime":{"end_time":"2023-06-02T04:52:43.041776+00:00","start_time":"2023-06-02T04:52:42.081512+00:00"}},"execution_count":null,"source":"# final check on the dataset and the annotation \n!ls /tmp/data","outputs":[]},{"id":"99755fee-50ce-41d3-bf2d-1aa219f387a4","cell_type":"code","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"noteable":{"cell_type":"code","output_collection_id":"8796ff2e-b26e-45c7-bdd2-a72065755747"},"ExecuteTime":{"end_time":"2023-06-02T05:49:05.193097+00:00","start_time":"2023-06-02T04:52:54.791433+00:00"},"scrolled":false},"execution_count":null,"source":"# state the filepath of the datasets and where the final model is saved\nMANIFEST_FILE_TRAIN = '/tmp/data/Train.csv'\nAUDIO_DIR_TRAIN = '/tmp/data/Train'\nSAVED_MODEL_PATH = '/tmp/data/model-freqtimemasking(50,120)_clayers4_rlayers6.pt'\n\n# simple check on the saved model path, will raise error if no directory found\nif not os.path.exists(os.path.dirname(SAVED_MODEL_PATH)):\n    raise FileNotFoundError\n\n# loads the dataset\ndataset = CustomSpeechDataset(\n    manifest_file=MANIFEST_FILE_TRAIN, \n    audio_dir=AUDIO_DIR_TRAIN, \n    is_test_set=False\n)\n\n# train_dev_split #changeable\ntrain_proportion = int(0.8 * len(dataset))\ndataset_train = list(dataset)[:train_proportion]\ndataset_dev = list(dataset)[train_proportion:]\n\n\nhparams = {\n        \"n_cnn_layers\": 4,      #changeable\n        \"n_rnn_layers\": 6,      #changeable\n        \"rnn_dim\": 512,         #changeable\n        \"n_class\": 28,          #26 alphabets in caps + <SPACE> + blanks\n        \"n_feats\": 128,\n        \"stride\": 2,            #changeable\n        \"dropout\": 0.1,         #changeable\n        \"learning_rate\": 5e-5,  #changeable\n        \"batch_size\": 8,        #changeable\n        \"epochs\": 50            #changeable\n  }\n\nstart_time = time()\n\n# start training the model\nmain(\n    hparams=hparams, \n    train_dataset=dataset_train, \n    dev_dataset=dataset_dev, \n    saved_model_path=SAVED_MODEL_PATH\n)\n\nend_time = time()\n\nprint(f\"Time taken for training: {(end_time-start_time)/(60*60)} hrs\")","outputs":[]},{"id":"66636f09-d198-4133-bb35-4323ac232f14","cell_type":"code","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"noteable":{"cell_type":"code","output_collection_id":"a3083990-f937-4de1-9e62-716369e9f4aa"},"ExecuteTime":{"end_time":"2023-06-02T05:49:14.283818+00:00","start_time":"2023-06-02T05:49:13.103623+00:00"}},"execution_count":null,"source":"# check if model.pt is produced\n!ls /tmp/data","outputs":[]},{"id":"404db212-888b-4be5-b553-13a760250943","cell_type":"code","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"noteable":{"cell_type":"code","output_collection_id":"80504128-8b50-42e4-b736-4c597f81e6ab"},"ExecuteTime":{"end_time":"2023-06-02T05:52:37.605286+00:00","start_time":"2023-06-02T05:52:36.479996+00:00"}},"execution_count":null,"source":"# copy the model over to datasets\n!cp /tmp/data/'model-freqtimemasking(50,120)_clayers4_rlayers6.pt' /etc/noteable/project/'model-freqtimemasking(50,120)_clayers4_rlayers6.pt'","outputs":[]},{"id":"20c13507-f260-47e0-a8fa-bb725263e9d6","cell_type":"code","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"noteable":{"cell_type":"code","output_collection_id":"e7549a0d-d886-48a3-979f-42a5e5a444b2"},"ExecuteTime":{"end_time":"2023-06-02T05:52:48.604071+00:00","start_time":"2023-06-02T05:52:47.552141+00:00"},"scrolled":true},"execution_count":null,"source":"!ls /etc/noteable/project/","outputs":[]},{"id":"9f3c3db9-367e-4e55-bc02-5902e5513daa","cell_type":"code","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"noteable":{"cell_type":"code"}},"execution_count":null,"source":"","outputs":[]},{"id":"ab4910ab-12bb-4e6a-9fcc-61b41c928063","cell_type":"markdown","source":"## Inference","metadata":{"noteable":{"cell_type":"markdown"}}},{"id":"111400b4-72cc-463b-ac8b-889d0a71babf","cell_type":"code","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"noteable":{"cell_type":"code","output_collection_id":"c87f2597-58fa-4a3f-a335-bcd4a9573cb3"},"ExecuteTime":{"end_time":"2023-06-02T05:53:22.487143+00:00","start_time":"2023-06-02T05:53:22.327992+00:00"}},"execution_count":null,"source":"def infer(hparams, test_dataset, model_path) -> Dict[str, str]:\n    \n    print('\\ngenerating inference ...')\n\n    use_cuda = torch.cuda.is_available()\n    torch.manual_seed(SEED)\n    \n    greedy_decoder = GreedyDecoder()\n    data_processor = DataProcessor()\n    \n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    kwargs = {'num_workers': 4, 'pin_memory': True} if use_cuda else {}\n\n    test_loader = torch.utils.data.DataLoader(\n        dataset=test_dataset,\n        batch_size=16,\n        shuffle=False,\n        collate_fn=lambda x: data_processor.data_processing(x, 'test'),\n        **kwargs\n    )\n    \n    # load the pretrained model\n    model = SpeechRecognitionModel(\n        hparams['n_cnn_layers'], \n        hparams['n_rnn_layers'], \n        hparams['rnn_dim'],\n        hparams['n_class'], \n        hparams['n_feats'], \n        hparams['stride'], \n        hparams['dropout']\n    ).to(device)\n    \n    model.load_state_dict(torch.load(model_path))\n    model.eval()\n    output_dict = {}\n    \n    with torch.no_grad():\n        for i, _data in tqdm(enumerate(test_loader)):\n            audio_path, spectrograms, input_lengths = _data\n            spectrograms = spectrograms.to(device)\n            output = model(spectrograms)  # (batch, time, n_class)\n            output = F.log_softmax(output, dim=2)\n            output = output.transpose(0, 1) # (time, batch, n_class) \n            decoded_preds_batch = greedy_decoder.decode(output.transpose(0, 1), labels=None, label_lengths=None, is_test=True)\n            \n            # batch prediction\n            for decoded_idx in range(len(decoded_preds_batch[0])):\n                output_dict[audio_path[decoded_idx]] = decoded_preds_batch[0][decoded_idx]\n                \n    print('done!\\n')\n    return output_dict","outputs":[]},{"id":"b7e8a29f-c25e-431a-b892-7a54e36da425","cell_type":"markdown","source":"## Calling the functions and classes to do model inference","metadata":{"noteable":{"cell_type":"markdown"}}},{"id":"ed010713-8bbb-48bc-97af-ad701a11ffdb","cell_type":"code","metadata":{"noteable":{"cell_type":"python","output_collection_id":"3dc0b2b8-7332-4a07-8c8d-fad82f078472"},"ExecuteTime":{"end_time":"2023-06-02T05:53:29.220601+00:00","start_time":"2023-06-02T05:53:28.147224+00:00"}},"execution_count":null,"source":"!ls /tmp/data","outputs":[]},{"id":"4cd27206-d75e-4d8a-8b9e-6dcd932a076b","cell_type":"code","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"noteable":{"cell_type":"code","output_collection_id":"32e9c509-be40-4afd-bf46-62124c71cd0f"},"ExecuteTime":{"end_time":"2023-06-02T05:56:28.569209+00:00","start_time":"2023-06-02T05:53:45.679951+00:00"},"scrolled":false},"execution_count":null,"source":"# same hyperparams as what you have used to train the model\nhparams = {\n        \"n_cnn_layers\": 4,\n        \"n_rnn_layers\": 6,\n        \"rnn_dim\": 512,\n        \"n_class\": 28, # 26 alphabets in caps + <SPACE> + blanks\n        \"n_feats\": 128,\n        \"stride\": 2,\n        \"dropout\": 0.1,\n        \"learning_rate\": None, # None, because parameter not used for inference\n        \"batch_size\": None, # None, because parameter not used for inference\n        \"epochs\": None # None, because parameter not used for inference\n    }\n\n# change the filepath as according\nSAVED_MODEL_PATH = '/tmp/data/model-freqtimemasking(50,120)_clayers4_rlayers6.pt'\nSUBMISSION_PATH = '/tmp/data/Submission_Advanced-freqtimemasking(50,120)_clayers4_rlayers6.csv'\n\nMANIFEST_FILE_TEST = '/tmp/data/Test_Advanced.csv' \nAUDIO_DIR_TEST = '/tmp/data/Test_Advanced/'\n\ndataset_test = CustomSpeechDataset(\n    manifest_file=MANIFEST_FILE_TEST, \n    audio_dir=AUDIO_DIR_TEST, \n    is_test_set=True\n)\n\nstart_time = time()\n\nsubmission_dict = infer(\n    hparams=hparams, \n    test_dataset=dataset_test, \n    model_path=SAVED_MODEL_PATH\n)\n\n# producing the final csv file for submission\nsubmission_list = []\n\nfor key in submission_dict:\n    submission_list.append(\n        {\n            \"path\": os.path.basename(key),\n            \"annotation\": submission_dict[key]\n        }\n    )\n\nsubmission_df = pd.DataFrame(submission_list)\nsubmission_df.to_csv(SUBMISSION_PATH, index=False)\n\nend_time = time()\n\nprint(f\"Time taken for inference: {(end_time-start_time)/60} min\")","outputs":[]},{"id":"956ba0f1-a985-464b-acaa-6e6ab62c0437","cell_type":"code","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"noteable":{"cell_type":"code","output_collection_id":"b8ab330b-c099-44d0-9fb8-48be63482e34"},"ExecuteTime":{"end_time":"2023-06-02T05:56:52.896168+00:00","start_time":"2023-06-02T05:56:51.817675+00:00"}},"execution_count":null,"source":"# check if the Submission_Advanced.csv exists\n!ls /tmp/data/","outputs":[]},{"id":"6f8a339f-f74a-4533-aa1a-2ab6c1b3c21d","cell_type":"code","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"noteable":{"cell_type":"code","output_collection_id":"335e4b23-730a-41e8-8bd9-1ba796123411"},"ExecuteTime":{"end_time":"2023-06-02T05:56:59.226172+00:00","start_time":"2023-06-02T05:56:58.099858+00:00"}},"execution_count":null,"source":"!cp /tmp/data/'Submission_Advanced-freqtimemasking(50,120)_clayers4_rlayers6.csv' /etc/noteable/project/","outputs":[]},{"id":"c8b3a47e-30ab-49d1-adfa-42eed8569521","cell_type":"code","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"noteable":{"cell_type":"code","output_collection_id":"018e9149-ee6a-49a2-89a7-62f7433704f8"},"ExecuteTime":{"end_time":"2023-06-02T05:57:05.508165+00:00","start_time":"2023-06-02T05:57:04.379868+00:00"}},"execution_count":null,"source":"!ls /etc/noteable/project/","outputs":[]},{"id":"9dc7652c-a556-4e6e-a004-413cf09d540a","cell_type":"code","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"noteable":{"cell_type":"code"}},"execution_count":null,"source":"","outputs":[]},{"id":"2d0fc6ae-0def-4aee-95f2-ea1acca222a2","cell_type":"code","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"noteable":{"cell_type":"code"}},"execution_count":null,"source":"","outputs":[]}]}